{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conorgibbons147/cyclegan-map/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf cyclegan-map # use if changes are made to the repo and you need to reclone"
      ],
      "metadata": {
        "id": "I2ynbzpE15M0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Qt8alUu2wR",
        "outputId": "5ed68972-a169-48ea-a9da-7dc8b181dcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cyclegan-map'...\n",
            "remote: Enumerating objects: 2687, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 2687 (delta 0), reused 13 (delta 0), pack-reused 2672 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2687/2687), 166.74 MiB | 13.42 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n",
            "Updating files: 100% (2624/2624), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/conorgibbons147/cyclegan-map.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "import sys\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "0Dpo_b-v2JNT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/cyclegan-map/models')\n",
        "from generator import Generator\n",
        "from discriminator import Discriminator"
      ],
      "metadata": {
        "id": "99Jln8OjxRDA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/cyclegan-map')\n",
        "from dataset import ImageDataset, HZDataset\n",
        "from utils import weight_init, ReplayBuffer, sample_images"
      ],
      "metadata": {
        "id": "DxNioAyQwyUV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 10\n",
        "batch_size = 1\n",
        "image_size = 256\n",
        "save_interval = 20  # how often to save images\n",
        "dataset_path = \"/content/cyclegan-map/data\""
      ],
      "metadata": {
        "id": "KiYYx7lz2nou"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create networks\n",
        "G_AB = Generator().to(device) # modern to vintage\n",
        "G_BA = Generator().to(device) # vintage to modern\n",
        "D_A = Discriminator().to(device) # check if fake vintage is real\n",
        "D_B = Discriminator().to(device) # check if fake modern is real\n",
        "\n",
        "# set weights for the networks\n",
        "G_AB.apply(weight_init)\n",
        "G_BA.apply(weight_init)\n",
        "D_A.apply(weight_init)\n",
        "D_B.apply(weight_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0lWqqcj2_c6",
        "outputId": "f917e4e3-e9a4-4705-937a-558c994f5222"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image loading setup\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BICUBIC), # how each image will be transformed to make them standard\n",
        "    transforms.ToTensor(),  # Converts image from [0,255] to [0.0,1.0]\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scales [0,1] to [-1,1]\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(dataset_path, transform=transform, mode='train')\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = ImageDataset(dataset_path, transform=transform, mode='val')\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "-A_NS_Xv5Awo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions\n",
        "criterion_GAN = nn.MSELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()\n",
        "\n",
        "lambda_cycle = 10.0               # lambdas are used to scale/weight the cycle and identity loss in our overall loss\n",
        "lambda_identity = 5.0"
      ],
      "metadata": {
        "id": "Zn8ypPlF551J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "QxdBzsve6AQ1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replay buffers - ensures that the discriminator is fed older saved fake images instead of just new ones, improves model performance\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()"
      ],
      "metadata": {
        "id": "1wV474mP6DWs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "    for i, batch in enumerate(train_loader): # form: 0 img1  1 img2  2 img3 (this is what enumerate does, saves index as i)\n",
        "        real_A = batch['A'].to(device)\n",
        "        real_B = batch['B'].to(device)\n",
        "\n",
        "        # ----- Generators -----\n",
        "        optimizer_G.zero_grad() # clears gradients before backpropogating on the new batch\n",
        "\n",
        "        # identity loss\n",
        "        same_B = G_AB(real_B)\n",
        "        loss_identity_B = criterion_identity(same_B, real_B) * lambda_identity # testing how different a vintage map becomes when fed into the modern->vintage generator,\n",
        "                                                                               # should be the same in theory\n",
        "        same_A = G_BA(real_A)\n",
        "        loss_identity_A = criterion_identity(same_A, real_A) * lambda_identity # same for modern map when fed into vintage->modern generator\n",
        "\n",
        "        # GAN loss - testing loss of the fake images compared to a completely real image\n",
        "        fake_B = G_AB(real_A)\n",
        "        pred_fake_B = D_B(fake_B)\n",
        "        loss_GAN_AB = criterion_GAN(pred_fake_B, torch.ones_like(pred_fake_B)) # torch.ones_like() creates a tensor of same shape as pred_fake_B full of ones, acts as a\n",
        "                                                                               # completely real image since generator outputs 1s when an image is deemed real, takes MSE loss\n",
        "        fake_A = G_BA(real_B)\n",
        "        pred_fake_A = D_A(fake_A)\n",
        "        loss_GAN_BA = criterion_GAN(pred_fake_A, torch.ones_like(pred_fake_A))\n",
        "\n",
        "        # cycle loss - loss when fake image is converted back to it's previous map type (ex. comparing original modern to modern->vinatage->modern duplicate)\n",
        "        recov_A = G_BA(fake_B)\n",
        "        loss_cycle_A = criterion_cycle(recov_A, real_A) * lambda_cycle\n",
        "\n",
        "        recov_B = G_AB(fake_A)\n",
        "        loss_cycle_B = criterion_cycle(recov_B, real_B) * lambda_cycle\n",
        "\n",
        "        # calculating total loss across the entire process\n",
        "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_AB + loss_GAN_BA + loss_cycle_A + loss_cycle_B\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ----- Discriminators -----\n",
        "        # A - testing how the real modern map compares to fake one\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        loss_real_A = criterion_GAN(D_A(real_A), torch.ones_like(D_A(real_A)))\n",
        "        fake_A_buffered = fake_A_buffer.push_and_pop(fake_A)\n",
        "        loss_fake_A = criterion_GAN(D_A(fake_A_buffered.detach()), torch.zeros_like(D_A(fake_A_buffered)))\n",
        "        loss_D_A = (loss_real_A + loss_fake_A) * 0.5\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # B - testing how the real vintage map compares to fake one\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        loss_real_B = criterion_GAN(D_B(real_B), torch.ones_like(D_B(real_B)))\n",
        "        fake_B_buffered = fake_B_buffer.push_and_pop(fake_B)\n",
        "        loss_fake_B = criterion_GAN(D_B(fake_B_buffered.detach()), torch.zeros_like(D_B(fake_B_buffered)))\n",
        "        loss_D_B = (loss_real_B + loss_fake_B) * 0.5\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        # print statement\n",
        "        print(f\"[Epoch {epoch+1}/{epochs}] [Batch {i+1}/{len(train_loader)}] \"\n",
        "              f\"[D_A loss: {loss_D_A.item():.4f}] [D_B loss: {loss_D_B.item():.4f}] [G loss: {loss_G.item():.4f}]\")\n",
        "\n",
        "        # saving images\n",
        "        batches_done = epoch * len(train_loader) + i\n",
        "        if batches_done % save_interval == 0:\n",
        "            sample_images(batches_done, G_AB, G_BA, val_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upUvzC8u6SsH",
        "outputId": "97e646ab-3f24-458b-c350-8c5d5d752311"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/10] [Batch 1/107] [D_A loss: 1.6308] [D_B loss: 1.9136] [G loss: 28.7131]\n",
            "[Epoch 1/10] [Batch 2/107] [D_A loss: 1.1466] [D_B loss: 1.5448] [G loss: 25.5577]\n",
            "[Epoch 1/10] [Batch 3/107] [D_A loss: 1.4857] [D_B loss: 1.5781] [G loss: 21.7955]\n",
            "[Epoch 1/10] [Batch 4/107] [D_A loss: 1.0988] [D_B loss: 1.5901] [G loss: 18.6709]\n",
            "[Epoch 1/10] [Batch 5/107] [D_A loss: 1.6940] [D_B loss: 5.3862] [G loss: 17.7035]\n",
            "[Epoch 1/10] [Batch 6/107] [D_A loss: 2.2379] [D_B loss: 1.6819] [G loss: 15.7480]\n",
            "[Epoch 1/10] [Batch 7/107] [D_A loss: 2.0347] [D_B loss: 1.6523] [G loss: 13.4765]\n",
            "[Epoch 1/10] [Batch 8/107] [D_A loss: 2.0024] [D_B loss: 1.8534] [G loss: 12.9674]\n",
            "[Epoch 1/10] [Batch 9/107] [D_A loss: 1.7018] [D_B loss: 1.3538] [G loss: 10.8177]\n",
            "[Epoch 1/10] [Batch 10/107] [D_A loss: 1.2374] [D_B loss: 1.3618] [G loss: 10.0569]\n",
            "[Epoch 1/10] [Batch 11/107] [D_A loss: 1.5871] [D_B loss: 1.4194] [G loss: 10.1438]\n",
            "[Epoch 1/10] [Batch 12/107] [D_A loss: 1.1720] [D_B loss: 1.3312] [G loss: 8.3800]\n",
            "[Epoch 1/10] [Batch 13/107] [D_A loss: 1.2025] [D_B loss: 1.4621] [G loss: 17.2297]\n",
            "[Epoch 1/10] [Batch 14/107] [D_A loss: 1.2443] [D_B loss: 1.3288] [G loss: 10.4745]\n",
            "[Epoch 1/10] [Batch 15/107] [D_A loss: 1.0622] [D_B loss: 1.0499] [G loss: 7.6712]\n",
            "[Epoch 1/10] [Batch 16/107] [D_A loss: 0.9168] [D_B loss: 1.3274] [G loss: 10.2488]\n",
            "[Epoch 1/10] [Batch 17/107] [D_A loss: 1.1368] [D_B loss: 1.3271] [G loss: 11.4977]\n",
            "[Epoch 1/10] [Batch 18/107] [D_A loss: 0.6824] [D_B loss: 0.9419] [G loss: 7.9017]\n",
            "[Epoch 1/10] [Batch 19/107] [D_A loss: 1.0172] [D_B loss: 0.9536] [G loss: 7.7949]\n",
            "[Epoch 1/10] [Batch 20/107] [D_A loss: 0.7343] [D_B loss: 1.0951] [G loss: 9.4429]\n",
            "[Epoch 1/10] [Batch 21/107] [D_A loss: 0.7593] [D_B loss: 1.2122] [G loss: 8.5526]\n",
            "[Epoch 1/10] [Batch 22/107] [D_A loss: 0.6264] [D_B loss: 1.2220] [G loss: 8.1344]\n",
            "[Epoch 1/10] [Batch 23/107] [D_A loss: 0.7099] [D_B loss: 1.3318] [G loss: 8.8856]\n",
            "[Epoch 1/10] [Batch 24/107] [D_A loss: 0.8347] [D_B loss: 1.0621] [G loss: 8.7147]\n",
            "[Epoch 1/10] [Batch 25/107] [D_A loss: 0.5468] [D_B loss: 1.0194] [G loss: 7.9865]\n",
            "[Epoch 1/10] [Batch 26/107] [D_A loss: 0.3787] [D_B loss: 1.0050] [G loss: 10.1220]\n",
            "[Epoch 1/10] [Batch 27/107] [D_A loss: 0.4728] [D_B loss: 1.0613] [G loss: 8.7968]\n",
            "[Epoch 1/10] [Batch 28/107] [D_A loss: 0.2854] [D_B loss: 0.8990] [G loss: 11.2025]\n",
            "[Epoch 1/10] [Batch 29/107] [D_A loss: 0.4295] [D_B loss: 0.8916] [G loss: 9.3393]\n",
            "[Epoch 1/10] [Batch 30/107] [D_A loss: 0.4164] [D_B loss: 0.8208] [G loss: 9.6770]\n",
            "[Epoch 1/10] [Batch 31/107] [D_A loss: 0.6161] [D_B loss: 0.7286] [G loss: 8.2702]\n",
            "[Epoch 1/10] [Batch 32/107] [D_A loss: 1.5311] [D_B loss: 1.0267] [G loss: 9.3840]\n",
            "[Epoch 1/10] [Batch 33/107] [D_A loss: 1.3989] [D_B loss: 1.3194] [G loss: 12.7814]\n",
            "[Epoch 1/10] [Batch 34/107] [D_A loss: 0.9633] [D_B loss: 1.0262] [G loss: 8.1970]\n",
            "[Epoch 1/10] [Batch 35/107] [D_A loss: 0.8644] [D_B loss: 0.8755] [G loss: 9.2071]\n",
            "[Epoch 1/10] [Batch 36/107] [D_A loss: 0.7691] [D_B loss: 1.1333] [G loss: 10.3473]\n",
            "[Epoch 1/10] [Batch 37/107] [D_A loss: 0.5718] [D_B loss: 1.0069] [G loss: 7.7550]\n",
            "[Epoch 1/10] [Batch 38/107] [D_A loss: 0.8675] [D_B loss: 0.8960] [G loss: 9.6474]\n",
            "[Epoch 1/10] [Batch 39/107] [D_A loss: 0.4415] [D_B loss: 0.7665] [G loss: 8.4092]\n",
            "[Epoch 1/10] [Batch 40/107] [D_A loss: 0.6705] [D_B loss: 0.8497] [G loss: 7.5104]\n",
            "[Epoch 1/10] [Batch 41/107] [D_A loss: 0.3255] [D_B loss: 1.0924] [G loss: 7.5344]\n",
            "[Epoch 1/10] [Batch 42/107] [D_A loss: 0.3125] [D_B loss: 0.9151] [G loss: 9.1620]\n",
            "[Epoch 1/10] [Batch 43/107] [D_A loss: 0.4312] [D_B loss: 0.7321] [G loss: 8.6161]\n",
            "[Epoch 1/10] [Batch 44/107] [D_A loss: 0.2518] [D_B loss: 0.9309] [G loss: 7.4891]\n",
            "[Epoch 1/10] [Batch 45/107] [D_A loss: 0.3205] [D_B loss: 1.4382] [G loss: 8.2562]\n",
            "[Epoch 1/10] [Batch 46/107] [D_A loss: 0.3128] [D_B loss: 1.0762] [G loss: 7.0183]\n",
            "[Epoch 1/10] [Batch 47/107] [D_A loss: 0.3125] [D_B loss: 0.7967] [G loss: 7.5611]\n",
            "[Epoch 1/10] [Batch 48/107] [D_A loss: 0.3464] [D_B loss: 1.6744] [G loss: 9.9263]\n",
            "[Epoch 1/10] [Batch 49/107] [D_A loss: 0.2546] [D_B loss: 1.8072] [G loss: 8.5515]\n",
            "[Epoch 1/10] [Batch 50/107] [D_A loss: 0.3536] [D_B loss: 1.3171] [G loss: 10.1164]\n",
            "[Epoch 1/10] [Batch 51/107] [D_A loss: 0.5450] [D_B loss: 0.8855] [G loss: 10.4344]\n",
            "[Epoch 1/10] [Batch 52/107] [D_A loss: 0.4613] [D_B loss: 0.8311] [G loss: 5.4338]\n",
            "[Epoch 1/10] [Batch 53/107] [D_A loss: 0.2911] [D_B loss: 0.8226] [G loss: 8.1387]\n",
            "[Epoch 1/10] [Batch 54/107] [D_A loss: 0.1854] [D_B loss: 0.9021] [G loss: 8.6989]\n",
            "[Epoch 1/10] [Batch 55/107] [D_A loss: 0.4732] [D_B loss: 1.3215] [G loss: 9.8788]\n",
            "[Epoch 1/10] [Batch 56/107] [D_A loss: 0.3397] [D_B loss: 0.6029] [G loss: 8.4783]\n",
            "[Epoch 1/10] [Batch 57/107] [D_A loss: 0.3835] [D_B loss: 0.8653] [G loss: 7.8098]\n",
            "[Epoch 1/10] [Batch 58/107] [D_A loss: 0.1225] [D_B loss: 1.7041] [G loss: 8.1550]\n",
            "[Epoch 1/10] [Batch 59/107] [D_A loss: 0.2073] [D_B loss: 1.0194] [G loss: 7.2093]\n",
            "[Epoch 1/10] [Batch 60/107] [D_A loss: 0.2345] [D_B loss: 0.6739] [G loss: 7.1515]\n",
            "[Epoch 1/10] [Batch 61/107] [D_A loss: 0.2464] [D_B loss: 0.7953] [G loss: 7.5718]\n",
            "[Epoch 1/10] [Batch 62/107] [D_A loss: 0.1483] [D_B loss: 1.6972] [G loss: 7.5669]\n",
            "[Epoch 1/10] [Batch 63/107] [D_A loss: 0.0986] [D_B loss: 1.6974] [G loss: 9.5775]\n",
            "[Epoch 1/10] [Batch 64/107] [D_A loss: 0.1522] [D_B loss: 0.7154] [G loss: 11.0571]\n",
            "[Epoch 1/10] [Batch 65/107] [D_A loss: 0.4175] [D_B loss: 1.1323] [G loss: 9.0851]\n",
            "[Epoch 1/10] [Batch 66/107] [D_A loss: 0.4091] [D_B loss: 0.6854] [G loss: 6.1521]\n",
            "[Epoch 1/10] [Batch 67/107] [D_A loss: 0.4427] [D_B loss: 0.8376] [G loss: 10.5163]\n",
            "[Epoch 1/10] [Batch 68/107] [D_A loss: 0.3754] [D_B loss: 0.7275] [G loss: 6.4894]\n",
            "[Epoch 1/10] [Batch 69/107] [D_A loss: 0.3320] [D_B loss: 0.9231] [G loss: 7.0851]\n",
            "[Epoch 1/10] [Batch 70/107] [D_A loss: 0.3213] [D_B loss: 0.9066] [G loss: 9.2614]\n",
            "[Epoch 1/10] [Batch 71/107] [D_A loss: 0.2033] [D_B loss: 0.6328] [G loss: 6.7681]\n",
            "[Epoch 1/10] [Batch 72/107] [D_A loss: 0.2725] [D_B loss: 0.7190] [G loss: 7.1519]\n",
            "[Epoch 1/10] [Batch 73/107] [D_A loss: 0.2088] [D_B loss: 0.6468] [G loss: 7.8492]\n",
            "[Epoch 1/10] [Batch 74/107] [D_A loss: 0.2071] [D_B loss: 0.5080] [G loss: 6.2475]\n",
            "[Epoch 1/10] [Batch 75/107] [D_A loss: 0.3182] [D_B loss: 0.5366] [G loss: 5.7819]\n",
            "[Epoch 1/10] [Batch 76/107] [D_A loss: 0.2190] [D_B loss: 0.7161] [G loss: 6.4012]\n",
            "[Epoch 1/10] [Batch 77/107] [D_A loss: 0.2280] [D_B loss: 0.4989] [G loss: 7.6963]\n",
            "[Epoch 1/10] [Batch 78/107] [D_A loss: 0.2018] [D_B loss: 0.5745] [G loss: 7.3538]\n",
            "[Epoch 1/10] [Batch 79/107] [D_A loss: 0.3188] [D_B loss: 0.5120] [G loss: 5.6413]\n",
            "[Epoch 1/10] [Batch 80/107] [D_A loss: 0.3082] [D_B loss: 0.5110] [G loss: 7.5638]\n",
            "[Epoch 1/10] [Batch 81/107] [D_A loss: 0.4235] [D_B loss: 0.5152] [G loss: 9.0592]\n",
            "[Epoch 1/10] [Batch 82/107] [D_A loss: 0.4544] [D_B loss: 1.1330] [G loss: 6.7350]\n",
            "[Epoch 1/10] [Batch 83/107] [D_A loss: 0.3786] [D_B loss: 0.6512] [G loss: 6.2436]\n",
            "[Epoch 1/10] [Batch 84/107] [D_A loss: 0.4776] [D_B loss: 0.5429] [G loss: 8.1474]\n",
            "[Epoch 1/10] [Batch 85/107] [D_A loss: 0.3005] [D_B loss: 0.4608] [G loss: 7.9787]\n",
            "[Epoch 1/10] [Batch 86/107] [D_A loss: 0.2807] [D_B loss: 0.5332] [G loss: 7.0539]\n",
            "[Epoch 1/10] [Batch 87/107] [D_A loss: 0.1823] [D_B loss: 0.5945] [G loss: 7.4248]\n",
            "[Epoch 1/10] [Batch 88/107] [D_A loss: 0.1644] [D_B loss: 0.5144] [G loss: 6.5256]\n",
            "[Epoch 1/10] [Batch 89/107] [D_A loss: 0.1240] [D_B loss: 0.4130] [G loss: 6.4294]\n",
            "[Epoch 1/10] [Batch 90/107] [D_A loss: 0.1391] [D_B loss: 0.4214] [G loss: 8.3042]\n",
            "[Epoch 1/10] [Batch 91/107] [D_A loss: 0.3499] [D_B loss: 0.4597] [G loss: 8.0086]\n",
            "[Epoch 1/10] [Batch 92/107] [D_A loss: 0.1215] [D_B loss: 0.4110] [G loss: 6.8709]\n",
            "[Epoch 1/10] [Batch 93/107] [D_A loss: 0.1489] [D_B loss: 0.6664] [G loss: 6.6968]\n",
            "[Epoch 1/10] [Batch 94/107] [D_A loss: 0.2133] [D_B loss: 0.6043] [G loss: 10.2580]\n",
            "[Epoch 1/10] [Batch 95/107] [D_A loss: 0.1478] [D_B loss: 0.5367] [G loss: 6.2436]\n",
            "[Epoch 1/10] [Batch 96/107] [D_A loss: 0.3316] [D_B loss: 0.3256] [G loss: 5.8115]\n",
            "[Epoch 1/10] [Batch 97/107] [D_A loss: 0.2103] [D_B loss: 0.5715] [G loss: 6.9960]\n",
            "[Epoch 1/10] [Batch 98/107] [D_A loss: 0.1888] [D_B loss: 0.4538] [G loss: 8.5723]\n",
            "[Epoch 1/10] [Batch 99/107] [D_A loss: 0.1604] [D_B loss: 0.3452] [G loss: 6.6563]\n",
            "[Epoch 1/10] [Batch 100/107] [D_A loss: 0.2327] [D_B loss: 0.5924] [G loss: 9.0458]\n",
            "[Epoch 1/10] [Batch 101/107] [D_A loss: 0.1817] [D_B loss: 0.4725] [G loss: 6.5668]\n",
            "[Epoch 1/10] [Batch 102/107] [D_A loss: 0.1711] [D_B loss: 0.5201] [G loss: 8.5307]\n",
            "[Epoch 1/10] [Batch 103/107] [D_A loss: 0.5432] [D_B loss: 0.3149] [G loss: 8.9814]\n",
            "[Epoch 1/10] [Batch 104/107] [D_A loss: 0.3967] [D_B loss: 0.4094] [G loss: 6.7440]\n",
            "[Epoch 1/10] [Batch 105/107] [D_A loss: 0.1458] [D_B loss: 0.4576] [G loss: 7.6075]\n",
            "[Epoch 1/10] [Batch 106/107] [D_A loss: 0.2865] [D_B loss: 0.3856] [G loss: 7.5216]\n",
            "[Epoch 1/10] [Batch 107/107] [D_A loss: 0.3574] [D_B loss: 0.2764] [G loss: 6.3621]\n",
            "[Epoch 2/10] [Batch 1/107] [D_A loss: 0.2695] [D_B loss: 0.3674] [G loss: 5.9580]\n",
            "[Epoch 2/10] [Batch 2/107] [D_A loss: 0.2484] [D_B loss: 0.5137] [G loss: 6.6173]\n",
            "[Epoch 2/10] [Batch 3/107] [D_A loss: 0.3995] [D_B loss: 0.4270] [G loss: 6.8082]\n",
            "[Epoch 2/10] [Batch 4/107] [D_A loss: 0.4283] [D_B loss: 0.5103] [G loss: 5.9052]\n",
            "[Epoch 2/10] [Batch 5/107] [D_A loss: 0.2935] [D_B loss: 0.2815] [G loss: 8.0810]\n",
            "[Epoch 2/10] [Batch 6/107] [D_A loss: 0.2400] [D_B loss: 0.3206] [G loss: 7.0462]\n",
            "[Epoch 2/10] [Batch 7/107] [D_A loss: 0.2704] [D_B loss: 0.2623] [G loss: 10.3003]\n",
            "[Epoch 2/10] [Batch 8/107] [D_A loss: 0.2612] [D_B loss: 0.5095] [G loss: 6.5265]\n",
            "[Epoch 2/10] [Batch 9/107] [D_A loss: 0.6161] [D_B loss: 0.8119] [G loss: 7.6599]\n",
            "[Epoch 2/10] [Batch 10/107] [D_A loss: 0.2462] [D_B loss: 0.3761] [G loss: 6.5002]\n",
            "[Epoch 2/10] [Batch 11/107] [D_A loss: 0.3098] [D_B loss: 0.4911] [G loss: 6.1668]\n",
            "[Epoch 2/10] [Batch 12/107] [D_A loss: 0.2669] [D_B loss: 0.3135] [G loss: 5.0007]\n",
            "[Epoch 2/10] [Batch 13/107] [D_A loss: 0.2882] [D_B loss: 0.4318] [G loss: 6.4362]\n",
            "[Epoch 2/10] [Batch 14/107] [D_A loss: 0.2919] [D_B loss: 0.3533] [G loss: 5.6998]\n",
            "[Epoch 2/10] [Batch 15/107] [D_A loss: 0.1644] [D_B loss: 0.2778] [G loss: 5.8846]\n",
            "[Epoch 2/10] [Batch 16/107] [D_A loss: 0.1781] [D_B loss: 0.2841] [G loss: 4.4789]\n",
            "[Epoch 2/10] [Batch 17/107] [D_A loss: 0.3197] [D_B loss: 0.4911] [G loss: 5.9676]\n",
            "[Epoch 2/10] [Batch 18/107] [D_A loss: 0.2120] [D_B loss: 0.2549] [G loss: 5.9782]\n",
            "[Epoch 2/10] [Batch 19/107] [D_A loss: 0.1580] [D_B loss: 0.3598] [G loss: 5.0641]\n",
            "[Epoch 2/10] [Batch 20/107] [D_A loss: 0.3735] [D_B loss: 0.2696] [G loss: 5.3499]\n",
            "[Epoch 2/10] [Batch 21/107] [D_A loss: 0.3351] [D_B loss: 0.2252] [G loss: 7.3109]\n",
            "[Epoch 2/10] [Batch 22/107] [D_A loss: 0.5043] [D_B loss: 0.2770] [G loss: 9.7593]\n",
            "[Epoch 2/10] [Batch 23/107] [D_A loss: 0.4326] [D_B loss: 0.1996] [G loss: 6.4646]\n",
            "[Epoch 2/10] [Batch 24/107] [D_A loss: 0.3068] [D_B loss: 0.2304] [G loss: 5.9039]\n",
            "[Epoch 2/10] [Batch 25/107] [D_A loss: 0.2692] [D_B loss: 0.1771] [G loss: 6.6470]\n",
            "[Epoch 2/10] [Batch 26/107] [D_A loss: 0.2217] [D_B loss: 0.3349] [G loss: 5.1085]\n",
            "[Epoch 2/10] [Batch 27/107] [D_A loss: 0.1905] [D_B loss: 0.2742] [G loss: 4.5946]\n",
            "[Epoch 2/10] [Batch 28/107] [D_A loss: 0.1342] [D_B loss: 0.2345] [G loss: 6.6832]\n",
            "[Epoch 2/10] [Batch 29/107] [D_A loss: 0.2213] [D_B loss: 0.3939] [G loss: 4.8239]\n",
            "[Epoch 2/10] [Batch 30/107] [D_A loss: 0.2102] [D_B loss: 0.3359] [G loss: 6.9318]\n",
            "[Epoch 2/10] [Batch 31/107] [D_A loss: 0.2206] [D_B loss: 0.2610] [G loss: 6.7463]\n",
            "[Epoch 2/10] [Batch 32/107] [D_A loss: 0.1696] [D_B loss: 0.2167] [G loss: 5.3056]\n",
            "[Epoch 2/10] [Batch 33/107] [D_A loss: 0.2553] [D_B loss: 0.2882] [G loss: 5.6833]\n",
            "[Epoch 2/10] [Batch 34/107] [D_A loss: 0.2292] [D_B loss: 0.2577] [G loss: 5.6439]\n",
            "[Epoch 2/10] [Batch 35/107] [D_A loss: 0.1990] [D_B loss: 0.2933] [G loss: 5.7711]\n",
            "[Epoch 2/10] [Batch 36/107] [D_A loss: 0.3337] [D_B loss: 0.1747] [G loss: 5.0874]\n",
            "[Epoch 2/10] [Batch 37/107] [D_A loss: 0.1721] [D_B loss: 0.2004] [G loss: 5.7680]\n",
            "[Epoch 2/10] [Batch 38/107] [D_A loss: 0.1250] [D_B loss: 0.3614] [G loss: 5.8153]\n",
            "[Epoch 2/10] [Batch 39/107] [D_A loss: 0.1053] [D_B loss: 0.2761] [G loss: 5.9696]\n",
            "[Epoch 2/10] [Batch 40/107] [D_A loss: 0.1979] [D_B loss: 0.2858] [G loss: 6.7290]\n",
            "[Epoch 2/10] [Batch 41/107] [D_A loss: 0.1987] [D_B loss: 0.3878] [G loss: 6.3323]\n",
            "[Epoch 2/10] [Batch 42/107] [D_A loss: 0.3284] [D_B loss: 0.7198] [G loss: 7.5170]\n",
            "[Epoch 2/10] [Batch 43/107] [D_A loss: 0.2136] [D_B loss: 0.3874] [G loss: 6.1914]\n",
            "[Epoch 2/10] [Batch 44/107] [D_A loss: 0.2143] [D_B loss: 0.4024] [G loss: 7.0925]\n",
            "[Epoch 2/10] [Batch 45/107] [D_A loss: 0.2508] [D_B loss: 0.2921] [G loss: 6.3691]\n",
            "[Epoch 2/10] [Batch 46/107] [D_A loss: 0.2311] [D_B loss: 0.3006] [G loss: 5.8345]\n",
            "[Epoch 2/10] [Batch 47/107] [D_A loss: 0.2073] [D_B loss: 0.2770] [G loss: 5.9648]\n",
            "[Epoch 2/10] [Batch 48/107] [D_A loss: 0.2418] [D_B loss: 0.3141] [G loss: 6.7140]\n",
            "[Epoch 2/10] [Batch 49/107] [D_A loss: 0.2733] [D_B loss: 0.2006] [G loss: 7.4236]\n",
            "[Epoch 2/10] [Batch 50/107] [D_A loss: 0.1145] [D_B loss: 0.2995] [G loss: 6.4410]\n",
            "[Epoch 2/10] [Batch 51/107] [D_A loss: 0.0946] [D_B loss: 0.1525] [G loss: 7.7197]\n",
            "[Epoch 2/10] [Batch 52/107] [D_A loss: 0.2593] [D_B loss: 0.2818] [G loss: 6.0003]\n",
            "[Epoch 2/10] [Batch 53/107] [D_A loss: 0.2406] [D_B loss: 0.4513] [G loss: 6.9972]\n",
            "[Epoch 2/10] [Batch 54/107] [D_A loss: 0.2282] [D_B loss: 0.4589] [G loss: 11.3104]\n",
            "[Epoch 2/10] [Batch 55/107] [D_A loss: 0.1327] [D_B loss: 0.5640] [G loss: 9.7051]\n",
            "[Epoch 2/10] [Batch 56/107] [D_A loss: 0.1537] [D_B loss: 0.3001] [G loss: 6.3904]\n",
            "[Epoch 2/10] [Batch 57/107] [D_A loss: 0.2103] [D_B loss: 0.3774] [G loss: 8.1933]\n",
            "[Epoch 2/10] [Batch 58/107] [D_A loss: 0.4749] [D_B loss: 0.3230] [G loss: 6.5009]\n",
            "[Epoch 2/10] [Batch 59/107] [D_A loss: 0.4268] [D_B loss: 0.3860] [G loss: 6.5954]\n",
            "[Epoch 2/10] [Batch 60/107] [D_A loss: 0.1582] [D_B loss: 0.2662] [G loss: 7.2623]\n",
            "[Epoch 2/10] [Batch 61/107] [D_A loss: 0.2371] [D_B loss: 0.2191] [G loss: 5.9999]\n",
            "[Epoch 2/10] [Batch 62/107] [D_A loss: 0.2366] [D_B loss: 0.2121] [G loss: 9.6392]\n",
            "[Epoch 2/10] [Batch 63/107] [D_A loss: 0.2769] [D_B loss: 0.2115] [G loss: 7.3783]\n",
            "[Epoch 2/10] [Batch 64/107] [D_A loss: 0.2042] [D_B loss: 0.2051] [G loss: 6.8484]\n",
            "[Epoch 2/10] [Batch 65/107] [D_A loss: 0.3141] [D_B loss: 0.3466] [G loss: 6.8066]\n",
            "[Epoch 2/10] [Batch 66/107] [D_A loss: 0.2225] [D_B loss: 0.2524] [G loss: 7.3061]\n",
            "[Epoch 2/10] [Batch 67/107] [D_A loss: 0.1608] [D_B loss: 0.1989] [G loss: 11.1616]\n",
            "[Epoch 2/10] [Batch 68/107] [D_A loss: 0.2459] [D_B loss: 0.3833] [G loss: 8.0202]\n",
            "[Epoch 2/10] [Batch 69/107] [D_A loss: 0.3620] [D_B loss: 0.2692] [G loss: 7.1172]\n",
            "[Epoch 2/10] [Batch 70/107] [D_A loss: 0.2861] [D_B loss: 0.2894] [G loss: 6.0617]\n",
            "[Epoch 2/10] [Batch 71/107] [D_A loss: 0.1954] [D_B loss: 0.1771] [G loss: 4.6566]\n",
            "[Epoch 2/10] [Batch 72/107] [D_A loss: 0.2218] [D_B loss: 0.1961] [G loss: 5.1236]\n",
            "[Epoch 2/10] [Batch 73/107] [D_A loss: 0.2927] [D_B loss: 0.1475] [G loss: 5.7300]\n",
            "[Epoch 2/10] [Batch 74/107] [D_A loss: 0.2113] [D_B loss: 0.1773] [G loss: 5.4588]\n",
            "[Epoch 2/10] [Batch 75/107] [D_A loss: 0.2529] [D_B loss: 0.4079] [G loss: 4.2097]\n",
            "[Epoch 2/10] [Batch 76/107] [D_A loss: 0.2009] [D_B loss: 0.2613] [G loss: 7.5283]\n",
            "[Epoch 2/10] [Batch 77/107] [D_A loss: 0.1968] [D_B loss: 0.2548] [G loss: 6.1992]\n",
            "[Epoch 2/10] [Batch 78/107] [D_A loss: 0.2198] [D_B loss: 0.6679] [G loss: 7.1108]\n",
            "[Epoch 2/10] [Batch 79/107] [D_A loss: 0.2086] [D_B loss: 0.3472] [G loss: 6.4796]\n",
            "[Epoch 2/10] [Batch 80/107] [D_A loss: 0.2915] [D_B loss: 0.1865] [G loss: 6.1615]\n",
            "[Epoch 2/10] [Batch 81/107] [D_A loss: 0.1275] [D_B loss: 0.1563] [G loss: 5.4099]\n",
            "[Epoch 2/10] [Batch 82/107] [D_A loss: 0.1832] [D_B loss: 0.2230] [G loss: 6.2605]\n",
            "[Epoch 2/10] [Batch 83/107] [D_A loss: 0.1434] [D_B loss: 0.1867] [G loss: 5.1727]\n",
            "[Epoch 2/10] [Batch 84/107] [D_A loss: 0.1058] [D_B loss: 0.3805] [G loss: 8.4743]\n",
            "[Epoch 2/10] [Batch 85/107] [D_A loss: 0.2893] [D_B loss: 0.2693] [G loss: 5.9141]\n",
            "[Epoch 2/10] [Batch 86/107] [D_A loss: 0.1529] [D_B loss: 0.2403] [G loss: 4.9465]\n",
            "[Epoch 2/10] [Batch 87/107] [D_A loss: 0.2046] [D_B loss: 0.2530] [G loss: 6.3958]\n",
            "[Epoch 2/10] [Batch 88/107] [D_A loss: 0.2000] [D_B loss: 0.2336] [G loss: 7.1712]\n",
            "[Epoch 2/10] [Batch 89/107] [D_A loss: 0.2326] [D_B loss: 0.4215] [G loss: 6.3879]\n",
            "[Epoch 2/10] [Batch 90/107] [D_A loss: 0.2655] [D_B loss: 0.3758] [G loss: 7.5919]\n",
            "[Epoch 2/10] [Batch 91/107] [D_A loss: 0.1183] [D_B loss: 0.4529] [G loss: 7.3648]\n",
            "[Epoch 2/10] [Batch 92/107] [D_A loss: 0.1714] [D_B loss: 0.2189] [G loss: 5.3278]\n",
            "[Epoch 2/10] [Batch 93/107] [D_A loss: 0.1018] [D_B loss: 0.1435] [G loss: 5.6748]\n",
            "[Epoch 2/10] [Batch 94/107] [D_A loss: 0.2371] [D_B loss: 0.2101] [G loss: 7.1364]\n",
            "[Epoch 2/10] [Batch 95/107] [D_A loss: 0.1563] [D_B loss: 0.2083] [G loss: 6.1626]\n",
            "[Epoch 2/10] [Batch 96/107] [D_A loss: 0.2656] [D_B loss: 0.2337] [G loss: 5.3927]\n",
            "[Epoch 2/10] [Batch 97/107] [D_A loss: 0.2751] [D_B loss: 0.4523] [G loss: 6.3211]\n",
            "[Epoch 2/10] [Batch 98/107] [D_A loss: 0.1512] [D_B loss: 0.2698] [G loss: 5.1152]\n",
            "[Epoch 2/10] [Batch 99/107] [D_A loss: 0.2569] [D_B loss: 0.2895] [G loss: 5.3295]\n",
            "[Epoch 2/10] [Batch 100/107] [D_A loss: 0.1866] [D_B loss: 0.2119] [G loss: 6.9639]\n",
            "[Epoch 2/10] [Batch 101/107] [D_A loss: 0.1380] [D_B loss: 0.2906] [G loss: 6.2175]\n",
            "[Epoch 2/10] [Batch 102/107] [D_A loss: 0.4106] [D_B loss: 0.2113] [G loss: 8.4022]\n",
            "[Epoch 2/10] [Batch 103/107] [D_A loss: 0.1643] [D_B loss: 0.3744] [G loss: 5.1141]\n",
            "[Epoch 2/10] [Batch 104/107] [D_A loss: 0.3300] [D_B loss: 0.1715] [G loss: 4.9222]\n",
            "[Epoch 2/10] [Batch 105/107] [D_A loss: 0.3765] [D_B loss: 0.3898] [G loss: 4.7261]\n",
            "[Epoch 2/10] [Batch 106/107] [D_A loss: 0.1671] [D_B loss: 0.3326] [G loss: 8.1752]\n",
            "[Epoch 2/10] [Batch 107/107] [D_A loss: 0.1359] [D_B loss: 0.5183] [G loss: 4.5399]\n",
            "[Epoch 3/10] [Batch 1/107] [D_A loss: 0.2950] [D_B loss: 0.3129] [G loss: 5.4115]\n",
            "[Epoch 3/10] [Batch 2/107] [D_A loss: 0.2472] [D_B loss: 0.2670] [G loss: 5.3725]\n",
            "[Epoch 3/10] [Batch 3/107] [D_A loss: 0.3290] [D_B loss: 0.1943] [G loss: 6.2587]\n",
            "[Epoch 3/10] [Batch 4/107] [D_A loss: 0.1666] [D_B loss: 0.1604] [G loss: 6.6025]\n",
            "[Epoch 3/10] [Batch 5/107] [D_A loss: 0.2855] [D_B loss: 0.2327] [G loss: 4.5295]\n",
            "[Epoch 3/10] [Batch 6/107] [D_A loss: 0.2088] [D_B loss: 0.2923] [G loss: 5.6114]\n",
            "[Epoch 3/10] [Batch 7/107] [D_A loss: 0.2356] [D_B loss: 0.1884] [G loss: 5.6850]\n",
            "[Epoch 3/10] [Batch 8/107] [D_A loss: 0.2113] [D_B loss: 0.1310] [G loss: 5.3907]\n",
            "[Epoch 3/10] [Batch 9/107] [D_A loss: 0.1182] [D_B loss: 0.1414] [G loss: 6.4720]\n",
            "[Epoch 3/10] [Batch 10/107] [D_A loss: 0.1618] [D_B loss: 0.1407] [G loss: 5.1263]\n",
            "[Epoch 3/10] [Batch 11/107] [D_A loss: 0.2151] [D_B loss: 0.2358] [G loss: 4.8275]\n",
            "[Epoch 3/10] [Batch 12/107] [D_A loss: 0.2260] [D_B loss: 0.1011] [G loss: 7.0464]\n",
            "[Epoch 3/10] [Batch 13/107] [D_A loss: 0.2086] [D_B loss: 0.1945] [G loss: 4.8200]\n",
            "[Epoch 3/10] [Batch 14/107] [D_A loss: 0.3399] [D_B loss: 0.2049] [G loss: 3.9473]\n",
            "[Epoch 3/10] [Batch 15/107] [D_A loss: 0.3148] [D_B loss: 0.2531] [G loss: 4.9452]\n",
            "[Epoch 3/10] [Batch 16/107] [D_A loss: 0.2150] [D_B loss: 0.3955] [G loss: 6.5237]\n",
            "[Epoch 3/10] [Batch 17/107] [D_A loss: 0.2415] [D_B loss: 0.2380] [G loss: 5.2776]\n",
            "[Epoch 3/10] [Batch 18/107] [D_A loss: 0.1696] [D_B loss: 0.1040] [G loss: 6.5119]\n",
            "[Epoch 3/10] [Batch 19/107] [D_A loss: 0.2177] [D_B loss: 0.1677] [G loss: 6.1573]\n",
            "[Epoch 3/10] [Batch 20/107] [D_A loss: 0.1246] [D_B loss: 0.2426] [G loss: 5.4672]\n",
            "[Epoch 3/10] [Batch 21/107] [D_A loss: 0.2385] [D_B loss: 0.1707] [G loss: 7.0666]\n",
            "[Epoch 3/10] [Batch 22/107] [D_A loss: 0.1754] [D_B loss: 0.2416] [G loss: 6.3609]\n",
            "[Epoch 3/10] [Batch 23/107] [D_A loss: 0.1514] [D_B loss: 0.3675] [G loss: 5.5578]\n",
            "[Epoch 3/10] [Batch 24/107] [D_A loss: 0.2111] [D_B loss: 0.1776] [G loss: 5.1983]\n",
            "[Epoch 3/10] [Batch 25/107] [D_A loss: 0.2207] [D_B loss: 0.4546] [G loss: 7.2012]\n",
            "[Epoch 3/10] [Batch 26/107] [D_A loss: 0.4531] [D_B loss: 0.1851] [G loss: 6.5723]\n",
            "[Epoch 3/10] [Batch 27/107] [D_A loss: 0.3355] [D_B loss: 0.2012] [G loss: 5.5957]\n",
            "[Epoch 3/10] [Batch 28/107] [D_A loss: 0.3034] [D_B loss: 0.0808] [G loss: 7.3327]\n",
            "[Epoch 3/10] [Batch 29/107] [D_A loss: 0.3747] [D_B loss: 0.1592] [G loss: 6.3906]\n",
            "[Epoch 3/10] [Batch 30/107] [D_A loss: 0.1249] [D_B loss: 0.2740] [G loss: 5.4625]\n",
            "[Epoch 3/10] [Batch 31/107] [D_A loss: 0.2265] [D_B loss: 0.3470] [G loss: 4.4888]\n",
            "[Epoch 3/10] [Batch 32/107] [D_A loss: 0.1968] [D_B loss: 0.1683] [G loss: 5.0287]\n",
            "[Epoch 3/10] [Batch 33/107] [D_A loss: 0.2362] [D_B loss: 0.2407] [G loss: 3.9781]\n",
            "[Epoch 3/10] [Batch 34/107] [D_A loss: 0.1929] [D_B loss: 0.1745] [G loss: 5.4337]\n",
            "[Epoch 3/10] [Batch 35/107] [D_A loss: 0.2170] [D_B loss: 0.2733] [G loss: 5.9649]\n",
            "[Epoch 3/10] [Batch 36/107] [D_A loss: 0.1733] [D_B loss: 0.1804] [G loss: 5.3185]\n",
            "[Epoch 3/10] [Batch 37/107] [D_A loss: 0.1685] [D_B loss: 0.3244] [G loss: 5.0298]\n",
            "[Epoch 3/10] [Batch 38/107] [D_A loss: 0.2686] [D_B loss: 0.1609] [G loss: 4.6032]\n",
            "[Epoch 3/10] [Batch 39/107] [D_A loss: 0.1726] [D_B loss: 0.3118] [G loss: 4.3853]\n",
            "[Epoch 3/10] [Batch 40/107] [D_A loss: 0.2264] [D_B loss: 0.1590] [G loss: 7.1871]\n",
            "[Epoch 3/10] [Batch 41/107] [D_A loss: 0.3198] [D_B loss: 0.3667] [G loss: 6.5081]\n",
            "[Epoch 3/10] [Batch 42/107] [D_A loss: 0.2585] [D_B loss: 0.2336] [G loss: 6.6805]\n",
            "[Epoch 3/10] [Batch 43/107] [D_A loss: 0.2181] [D_B loss: 0.2703] [G loss: 5.9179]\n",
            "[Epoch 3/10] [Batch 44/107] [D_A loss: 0.1895] [D_B loss: 0.1135] [G loss: 5.3375]\n",
            "[Epoch 3/10] [Batch 45/107] [D_A loss: 0.1869] [D_B loss: 0.3958] [G loss: 5.5062]\n",
            "[Epoch 3/10] [Batch 46/107] [D_A loss: 0.2582] [D_B loss: 0.0979] [G loss: 4.7204]\n",
            "[Epoch 3/10] [Batch 47/107] [D_A loss: 0.0821] [D_B loss: 0.2920] [G loss: 5.0382]\n",
            "[Epoch 3/10] [Batch 48/107] [D_A loss: 0.2629] [D_B loss: 0.1690] [G loss: 4.9935]\n",
            "[Epoch 3/10] [Batch 49/107] [D_A loss: 0.1660] [D_B loss: 0.3754] [G loss: 8.3149]\n",
            "[Epoch 3/10] [Batch 50/107] [D_A loss: 0.3372] [D_B loss: 0.1408] [G loss: 7.4381]\n",
            "[Epoch 3/10] [Batch 51/107] [D_A loss: 0.2332] [D_B loss: 0.1314] [G loss: 6.7928]\n",
            "[Epoch 3/10] [Batch 52/107] [D_A loss: 0.2268] [D_B loss: 0.2495] [G loss: 9.1898]\n",
            "[Epoch 3/10] [Batch 53/107] [D_A loss: 0.3181] [D_B loss: 0.2900] [G loss: 6.2064]\n",
            "[Epoch 3/10] [Batch 54/107] [D_A loss: 0.2041] [D_B loss: 0.1740] [G loss: 6.0861]\n",
            "[Epoch 3/10] [Batch 55/107] [D_A loss: 0.1372] [D_B loss: 0.1818] [G loss: 5.1287]\n",
            "[Epoch 3/10] [Batch 56/107] [D_A loss: 0.2446] [D_B loss: 0.5052] [G loss: 4.7607]\n",
            "[Epoch 3/10] [Batch 57/107] [D_A loss: 0.3698] [D_B loss: 0.1745] [G loss: 5.2784]\n",
            "[Epoch 3/10] [Batch 58/107] [D_A loss: 0.2020] [D_B loss: 0.2219] [G loss: 5.6270]\n",
            "[Epoch 3/10] [Batch 59/107] [D_A loss: 0.1707] [D_B loss: 0.2740] [G loss: 5.4110]\n",
            "[Epoch 3/10] [Batch 60/107] [D_A loss: 0.2911] [D_B loss: 0.1582] [G loss: 5.0834]\n",
            "[Epoch 3/10] [Batch 61/107] [D_A loss: 0.1660] [D_B loss: 0.2162] [G loss: 5.2633]\n",
            "[Epoch 3/10] [Batch 62/107] [D_A loss: 0.2713] [D_B loss: 0.3138] [G loss: 4.3267]\n",
            "[Epoch 3/10] [Batch 63/107] [D_A loss: 0.1404] [D_B loss: 0.2049] [G loss: 8.2567]\n",
            "[Epoch 3/10] [Batch 64/107] [D_A loss: 0.1574] [D_B loss: 0.2529] [G loss: 5.6006]\n",
            "[Epoch 3/10] [Batch 65/107] [D_A loss: 0.3973] [D_B loss: 0.3446] [G loss: 4.2271]\n",
            "[Epoch 3/10] [Batch 66/107] [D_A loss: 0.2603] [D_B loss: 0.2311] [G loss: 4.3224]\n",
            "[Epoch 3/10] [Batch 67/107] [D_A loss: 0.2537] [D_B loss: 0.5153] [G loss: 6.5860]\n",
            "[Epoch 3/10] [Batch 68/107] [D_A loss: 0.2313] [D_B loss: 0.3273] [G loss: 4.9374]\n",
            "[Epoch 3/10] [Batch 69/107] [D_A loss: 0.1520] [D_B loss: 0.3219] [G loss: 6.0762]\n",
            "[Epoch 3/10] [Batch 70/107] [D_A loss: 0.2421] [D_B loss: 0.1970] [G loss: 6.2926]\n",
            "[Epoch 3/10] [Batch 71/107] [D_A loss: 0.1316] [D_B loss: 0.2371] [G loss: 4.8106]\n",
            "[Epoch 3/10] [Batch 72/107] [D_A loss: 0.2338] [D_B loss: 0.5390] [G loss: 7.0961]\n",
            "[Epoch 3/10] [Batch 73/107] [D_A loss: 0.2162] [D_B loss: 0.1653] [G loss: 6.2197]\n",
            "[Epoch 3/10] [Batch 74/107] [D_A loss: 0.1338] [D_B loss: 0.1941] [G loss: 6.5942]\n",
            "[Epoch 3/10] [Batch 75/107] [D_A loss: 0.1972] [D_B loss: 0.5030] [G loss: 4.9631]\n",
            "[Epoch 3/10] [Batch 76/107] [D_A loss: 0.2175] [D_B loss: 0.1645] [G loss: 6.3413]\n",
            "[Epoch 3/10] [Batch 77/107] [D_A loss: 0.0894] [D_B loss: 0.3393] [G loss: 3.6357]\n",
            "[Epoch 3/10] [Batch 78/107] [D_A loss: 0.0971] [D_B loss: 0.2245] [G loss: 5.6560]\n",
            "[Epoch 3/10] [Batch 79/107] [D_A loss: 0.2819] [D_B loss: 0.1536] [G loss: 11.6591]\n",
            "[Epoch 3/10] [Batch 80/107] [D_A loss: 0.2290] [D_B loss: 0.5426] [G loss: 7.1674]\n",
            "[Epoch 3/10] [Batch 81/107] [D_A loss: 0.3846] [D_B loss: 0.3385] [G loss: 4.7038]\n",
            "[Epoch 3/10] [Batch 82/107] [D_A loss: 0.2128] [D_B loss: 0.3389] [G loss: 5.6912]\n",
            "[Epoch 3/10] [Batch 83/107] [D_A loss: 0.2910] [D_B loss: 0.1896] [G loss: 5.3805]\n",
            "[Epoch 3/10] [Batch 84/107] [D_A loss: 0.1092] [D_B loss: 0.2500] [G loss: 5.2013]\n",
            "[Epoch 3/10] [Batch 85/107] [D_A loss: 0.0958] [D_B loss: 0.2999] [G loss: 7.5227]\n",
            "[Epoch 3/10] [Batch 86/107] [D_A loss: 0.2414] [D_B loss: 0.2323] [G loss: 4.7084]\n",
            "[Epoch 3/10] [Batch 87/107] [D_A loss: 0.2772] [D_B loss: 0.2259] [G loss: 5.1495]\n",
            "[Epoch 3/10] [Batch 88/107] [D_A loss: 0.1471] [D_B loss: 0.1802] [G loss: 7.9551]\n",
            "[Epoch 3/10] [Batch 89/107] [D_A loss: 0.2559] [D_B loss: 0.2169] [G loss: 5.4797]\n",
            "[Epoch 3/10] [Batch 90/107] [D_A loss: 0.2228] [D_B loss: 0.2747] [G loss: 6.1252]\n",
            "[Epoch 3/10] [Batch 91/107] [D_A loss: 0.1014] [D_B loss: 0.1715] [G loss: 7.6035]\n",
            "[Epoch 3/10] [Batch 92/107] [D_A loss: 0.3024] [D_B loss: 0.3498] [G loss: 6.1650]\n",
            "[Epoch 3/10] [Batch 93/107] [D_A loss: 0.1767] [D_B loss: 0.2170] [G loss: 5.4537]\n",
            "[Epoch 3/10] [Batch 94/107] [D_A loss: 0.3102] [D_B loss: 0.2077] [G loss: 6.1613]\n",
            "[Epoch 3/10] [Batch 95/107] [D_A loss: 0.2307] [D_B loss: 0.5147] [G loss: 7.6757]\n",
            "[Epoch 3/10] [Batch 96/107] [D_A loss: 0.2645] [D_B loss: 0.1488] [G loss: 4.7778]\n",
            "[Epoch 3/10] [Batch 97/107] [D_A loss: 0.1525] [D_B loss: 0.3123] [G loss: 4.5449]\n",
            "[Epoch 3/10] [Batch 98/107] [D_A loss: 0.2457] [D_B loss: 0.1473] [G loss: 7.6252]\n",
            "[Epoch 3/10] [Batch 99/107] [D_A loss: 0.2355] [D_B loss: 0.1417] [G loss: 6.7488]\n",
            "[Epoch 3/10] [Batch 100/107] [D_A loss: 0.1770] [D_B loss: 0.2544] [G loss: 4.9089]\n",
            "[Epoch 3/10] [Batch 101/107] [D_A loss: 0.2691] [D_B loss: 0.1411] [G loss: 5.0870]\n",
            "[Epoch 3/10] [Batch 102/107] [D_A loss: 0.2666] [D_B loss: 0.7522] [G loss: 6.0147]\n",
            "[Epoch 3/10] [Batch 103/107] [D_A loss: 0.2128] [D_B loss: 0.1999] [G loss: 6.3828]\n",
            "[Epoch 3/10] [Batch 104/107] [D_A loss: 0.2027] [D_B loss: 0.4524] [G loss: 7.8816]\n",
            "[Epoch 3/10] [Batch 105/107] [D_A loss: 0.0931] [D_B loss: 0.2792] [G loss: 5.0384]\n",
            "[Epoch 3/10] [Batch 106/107] [D_A loss: 0.2956] [D_B loss: 0.4612] [G loss: 4.9773]\n",
            "[Epoch 3/10] [Batch 107/107] [D_A loss: 0.1698] [D_B loss: 0.5115] [G loss: 7.1678]\n",
            "[Epoch 4/10] [Batch 1/107] [D_A loss: 0.2046] [D_B loss: 0.3601] [G loss: 13.0174]\n",
            "[Epoch 4/10] [Batch 2/107] [D_A loss: 0.1801] [D_B loss: 0.5588] [G loss: 8.6682]\n",
            "[Epoch 4/10] [Batch 3/107] [D_A loss: 0.2860] [D_B loss: 0.3937] [G loss: 6.2793]\n",
            "[Epoch 4/10] [Batch 4/107] [D_A loss: 0.2855] [D_B loss: 0.2615] [G loss: 4.7298]\n",
            "[Epoch 4/10] [Batch 5/107] [D_A loss: 0.0865] [D_B loss: 0.3090] [G loss: 5.6201]\n",
            "[Epoch 4/10] [Batch 6/107] [D_A loss: 0.0927] [D_B loss: 0.2032] [G loss: 6.2386]\n",
            "[Epoch 4/10] [Batch 7/107] [D_A loss: 0.2313] [D_B loss: 0.2261] [G loss: 6.3293]\n",
            "[Epoch 4/10] [Batch 8/107] [D_A loss: 0.3003] [D_B loss: 0.1830] [G loss: 5.3068]\n",
            "[Epoch 4/10] [Batch 9/107] [D_A loss: 0.1275] [D_B loss: 0.3145] [G loss: 5.3856]\n",
            "[Epoch 4/10] [Batch 10/107] [D_A loss: 0.1381] [D_B loss: 0.2553] [G loss: 8.2377]\n",
            "[Epoch 4/10] [Batch 11/107] [D_A loss: 0.0919] [D_B loss: 0.1709] [G loss: 5.2554]\n",
            "[Epoch 4/10] [Batch 12/107] [D_A loss: 0.1123] [D_B loss: 0.1574] [G loss: 10.5496]\n",
            "[Epoch 4/10] [Batch 13/107] [D_A loss: 0.1315] [D_B loss: 0.1372] [G loss: 5.0636]\n",
            "[Epoch 4/10] [Batch 14/107] [D_A loss: 0.3316] [D_B loss: 0.0970] [G loss: 4.7012]\n",
            "[Epoch 4/10] [Batch 15/107] [D_A loss: 0.0761] [D_B loss: 0.1430] [G loss: 6.6482]\n",
            "[Epoch 4/10] [Batch 16/107] [D_A loss: 0.4560] [D_B loss: 0.1623] [G loss: 5.1446]\n",
            "[Epoch 4/10] [Batch 17/107] [D_A loss: 0.1907] [D_B loss: 0.2134] [G loss: 4.4997]\n",
            "[Epoch 4/10] [Batch 18/107] [D_A loss: 0.1015] [D_B loss: 0.2692] [G loss: 4.1862]\n",
            "[Epoch 4/10] [Batch 19/107] [D_A loss: 0.1233] [D_B loss: 0.3074] [G loss: 4.4237]\n",
            "[Epoch 4/10] [Batch 20/107] [D_A loss: 0.2645] [D_B loss: 0.2424] [G loss: 4.6303]\n",
            "[Epoch 4/10] [Batch 21/107] [D_A loss: 0.1487] [D_B loss: 0.2560] [G loss: 6.8851]\n",
            "[Epoch 4/10] [Batch 22/107] [D_A loss: 0.1919] [D_B loss: 0.1875] [G loss: 6.5313]\n",
            "[Epoch 4/10] [Batch 23/107] [D_A loss: 0.3015] [D_B loss: 0.2119] [G loss: 4.6603]\n",
            "[Epoch 4/10] [Batch 24/107] [D_A loss: 0.1984] [D_B loss: 0.2161] [G loss: 4.6146]\n",
            "[Epoch 4/10] [Batch 25/107] [D_A loss: 0.1993] [D_B loss: 0.1134] [G loss: 5.0406]\n",
            "[Epoch 4/10] [Batch 26/107] [D_A loss: 0.4702] [D_B loss: 0.1815] [G loss: 4.7968]\n",
            "[Epoch 4/10] [Batch 27/107] [D_A loss: 0.1424] [D_B loss: 0.2300] [G loss: 6.1172]\n",
            "[Epoch 4/10] [Batch 28/107] [D_A loss: 0.0731] [D_B loss: 0.1691] [G loss: 5.8674]\n",
            "[Epoch 4/10] [Batch 29/107] [D_A loss: 0.1733] [D_B loss: 0.2868] [G loss: 5.5081]\n",
            "[Epoch 4/10] [Batch 30/107] [D_A loss: 0.2315] [D_B loss: 0.3336] [G loss: 8.5995]\n",
            "[Epoch 4/10] [Batch 31/107] [D_A loss: 0.2120] [D_B loss: 0.1460] [G loss: 7.7634]\n",
            "[Epoch 4/10] [Batch 32/107] [D_A loss: 0.4290] [D_B loss: 0.0999] [G loss: 5.1349]\n",
            "[Epoch 4/10] [Batch 33/107] [D_A loss: 0.2827] [D_B loss: 0.1972] [G loss: 7.7604]\n",
            "[Epoch 4/10] [Batch 34/107] [D_A loss: 0.2066] [D_B loss: 0.1155] [G loss: 5.5115]\n",
            "[Epoch 4/10] [Batch 35/107] [D_A loss: 0.2371] [D_B loss: 0.1938] [G loss: 5.1232]\n",
            "[Epoch 4/10] [Batch 36/107] [D_A loss: 0.1615] [D_B loss: 0.1667] [G loss: 4.5700]\n",
            "[Epoch 4/10] [Batch 37/107] [D_A loss: 0.1253] [D_B loss: 0.3743] [G loss: 4.1038]\n",
            "[Epoch 4/10] [Batch 38/107] [D_A loss: 0.1180] [D_B loss: 0.1678] [G loss: 5.0946]\n",
            "[Epoch 4/10] [Batch 39/107] [D_A loss: 0.2017] [D_B loss: 0.3294] [G loss: 5.0246]\n",
            "[Epoch 4/10] [Batch 40/107] [D_A loss: 0.2286] [D_B loss: 0.2861] [G loss: 5.4303]\n",
            "[Epoch 4/10] [Batch 41/107] [D_A loss: 0.2737] [D_B loss: 0.1500] [G loss: 5.2380]\n",
            "[Epoch 4/10] [Batch 42/107] [D_A loss: 0.2896] [D_B loss: 0.1447] [G loss: 7.0581]\n",
            "[Epoch 4/10] [Batch 43/107] [D_A loss: 0.2367] [D_B loss: 0.2045] [G loss: 4.6902]\n",
            "[Epoch 4/10] [Batch 44/107] [D_A loss: 0.3633] [D_B loss: 0.1783] [G loss: 5.7935]\n",
            "[Epoch 4/10] [Batch 45/107] [D_A loss: 0.2406] [D_B loss: 0.1868] [G loss: 4.8231]\n",
            "[Epoch 4/10] [Batch 46/107] [D_A loss: 0.0894] [D_B loss: 0.1943] [G loss: 7.0659]\n",
            "[Epoch 4/10] [Batch 47/107] [D_A loss: 0.1748] [D_B loss: 0.1906] [G loss: 6.6083]\n",
            "[Epoch 4/10] [Batch 48/107] [D_A loss: 0.3211] [D_B loss: 0.1187] [G loss: 4.4977]\n",
            "[Epoch 4/10] [Batch 49/107] [D_A loss: 0.2138] [D_B loss: 0.1675] [G loss: 5.8327]\n",
            "[Epoch 4/10] [Batch 50/107] [D_A loss: 0.1387] [D_B loss: 0.0927] [G loss: 6.3305]\n",
            "[Epoch 4/10] [Batch 51/107] [D_A loss: 0.2252] [D_B loss: 0.1120] [G loss: 6.5354]\n",
            "[Epoch 4/10] [Batch 52/107] [D_A loss: 0.5466] [D_B loss: 0.0602] [G loss: 5.4929]\n",
            "[Epoch 4/10] [Batch 53/107] [D_A loss: 0.5308] [D_B loss: 0.1471] [G loss: 5.8830]\n",
            "[Epoch 4/10] [Batch 54/107] [D_A loss: 0.2587] [D_B loss: 0.2990] [G loss: 6.8344]\n",
            "[Epoch 4/10] [Batch 55/107] [D_A loss: 0.1202] [D_B loss: 0.2576] [G loss: 5.6321]\n",
            "[Epoch 4/10] [Batch 56/107] [D_A loss: 0.2780] [D_B loss: 0.1726] [G loss: 5.6454]\n",
            "[Epoch 4/10] [Batch 57/107] [D_A loss: 0.2980] [D_B loss: 0.1159] [G loss: 5.4452]\n",
            "[Epoch 4/10] [Batch 58/107] [D_A loss: 0.0889] [D_B loss: 0.2484] [G loss: 4.8789]\n",
            "[Epoch 4/10] [Batch 59/107] [D_A loss: 0.2150] [D_B loss: 0.2699] [G loss: 4.4089]\n",
            "[Epoch 4/10] [Batch 60/107] [D_A loss: 0.2960] [D_B loss: 0.2593] [G loss: 4.8463]\n",
            "[Epoch 4/10] [Batch 61/107] [D_A loss: 0.2727] [D_B loss: 0.1629] [G loss: 5.0688]\n",
            "[Epoch 4/10] [Batch 62/107] [D_A loss: 0.2149] [D_B loss: 0.1613] [G loss: 4.4178]\n",
            "[Epoch 4/10] [Batch 63/107] [D_A loss: 0.1452] [D_B loss: 0.0654] [G loss: 6.9633]\n",
            "[Epoch 4/10] [Batch 64/107] [D_A loss: 0.2452] [D_B loss: 0.3213] [G loss: 5.7426]\n",
            "[Epoch 4/10] [Batch 65/107] [D_A loss: 0.1621] [D_B loss: 0.2241] [G loss: 5.0285]\n",
            "[Epoch 4/10] [Batch 66/107] [D_A loss: 0.1076] [D_B loss: 0.4651] [G loss: 4.7224]\n",
            "[Epoch 4/10] [Batch 67/107] [D_A loss: 0.2870] [D_B loss: 0.3150] [G loss: 7.4501]\n",
            "[Epoch 4/10] [Batch 68/107] [D_A loss: 0.2365] [D_B loss: 0.3739] [G loss: 5.8867]\n",
            "[Epoch 4/10] [Batch 69/107] [D_A loss: 0.1292] [D_B loss: 0.1857] [G loss: 6.8797]\n",
            "[Epoch 4/10] [Batch 70/107] [D_A loss: 0.1961] [D_B loss: 0.1769] [G loss: 5.6963]\n",
            "[Epoch 4/10] [Batch 71/107] [D_A loss: 0.1848] [D_B loss: 0.1968] [G loss: 4.8712]\n",
            "[Epoch 4/10] [Batch 72/107] [D_A loss: 0.1552] [D_B loss: 0.0799] [G loss: 7.4036]\n",
            "[Epoch 4/10] [Batch 73/107] [D_A loss: 0.2610] [D_B loss: 0.3150] [G loss: 5.4154]\n",
            "[Epoch 4/10] [Batch 74/107] [D_A loss: 0.1817] [D_B loss: 0.1815] [G loss: 6.1084]\n",
            "[Epoch 4/10] [Batch 75/107] [D_A loss: 0.2547] [D_B loss: 0.2425] [G loss: 5.2663]\n",
            "[Epoch 4/10] [Batch 76/107] [D_A loss: 0.1772] [D_B loss: 0.2805] [G loss: 5.3157]\n",
            "[Epoch 4/10] [Batch 77/107] [D_A loss: 0.1484] [D_B loss: 0.5260] [G loss: 6.1670]\n",
            "[Epoch 4/10] [Batch 78/107] [D_A loss: 0.2483] [D_B loss: 0.1090] [G loss: 4.1642]\n",
            "[Epoch 4/10] [Batch 79/107] [D_A loss: 0.0767] [D_B loss: 0.3593] [G loss: 4.8240]\n",
            "[Epoch 4/10] [Batch 80/107] [D_A loss: 0.2294] [D_B loss: 0.1585] [G loss: 5.1357]\n",
            "[Epoch 4/10] [Batch 81/107] [D_A loss: 0.1172] [D_B loss: 0.1268] [G loss: 5.0662]\n",
            "[Epoch 4/10] [Batch 82/107] [D_A loss: 0.2054] [D_B loss: 0.2637] [G loss: 5.8171]\n",
            "[Epoch 4/10] [Batch 83/107] [D_A loss: 0.2106] [D_B loss: 0.2417] [G loss: 7.3285]\n",
            "[Epoch 4/10] [Batch 84/107] [D_A loss: 0.1275] [D_B loss: 0.1133] [G loss: 8.6508]\n",
            "[Epoch 4/10] [Batch 85/107] [D_A loss: 0.3190] [D_B loss: 0.3632] [G loss: 4.9884]\n",
            "[Epoch 4/10] [Batch 86/107] [D_A loss: 0.3312] [D_B loss: 0.4222] [G loss: 6.6957]\n",
            "[Epoch 4/10] [Batch 87/107] [D_A loss: 0.2480] [D_B loss: 0.3408] [G loss: 5.5723]\n",
            "[Epoch 4/10] [Batch 88/107] [D_A loss: 0.2226] [D_B loss: 0.1101] [G loss: 5.2071]\n",
            "[Epoch 4/10] [Batch 89/107] [D_A loss: 0.2548] [D_B loss: 0.2265] [G loss: 4.0061]\n",
            "[Epoch 4/10] [Batch 90/107] [D_A loss: 0.1198] [D_B loss: 0.3020] [G loss: 6.0212]\n",
            "[Epoch 4/10] [Batch 91/107] [D_A loss: 0.2878] [D_B loss: 0.2051] [G loss: 5.0225]\n",
            "[Epoch 4/10] [Batch 92/107] [D_A loss: 0.1862] [D_B loss: 0.3442] [G loss: 5.4954]\n",
            "[Epoch 4/10] [Batch 93/107] [D_A loss: 0.0734] [D_B loss: 0.2716] [G loss: 7.0070]\n",
            "[Epoch 4/10] [Batch 94/107] [D_A loss: 0.1088] [D_B loss: 0.1989] [G loss: 7.2549]\n",
            "[Epoch 4/10] [Batch 95/107] [D_A loss: 0.1797] [D_B loss: 0.3394] [G loss: 5.9069]\n",
            "[Epoch 4/10] [Batch 96/107] [D_A loss: 0.1658] [D_B loss: 0.3517] [G loss: 5.6197]\n",
            "[Epoch 4/10] [Batch 97/107] [D_A loss: 0.3190] [D_B loss: 0.2496] [G loss: 9.1361]\n",
            "[Epoch 4/10] [Batch 98/107] [D_A loss: 0.1816] [D_B loss: 0.1739] [G loss: 5.0440]\n",
            "[Epoch 4/10] [Batch 99/107] [D_A loss: 0.2394] [D_B loss: 0.3464] [G loss: 4.8990]\n",
            "[Epoch 4/10] [Batch 100/107] [D_A loss: 0.1893] [D_B loss: 0.0878] [G loss: 4.7575]\n",
            "[Epoch 4/10] [Batch 101/107] [D_A loss: 0.1781] [D_B loss: 0.3963] [G loss: 7.6434]\n",
            "[Epoch 4/10] [Batch 102/107] [D_A loss: 0.0867] [D_B loss: 0.4039] [G loss: 6.1027]\n",
            "[Epoch 4/10] [Batch 103/107] [D_A loss: 0.3505] [D_B loss: 0.2413] [G loss: 5.4608]\n",
            "[Epoch 4/10] [Batch 104/107] [D_A loss: 0.0909] [D_B loss: 0.2972] [G loss: 4.1497]\n",
            "[Epoch 4/10] [Batch 105/107] [D_A loss: 0.2554] [D_B loss: 0.2990] [G loss: 6.5161]\n",
            "[Epoch 4/10] [Batch 106/107] [D_A loss: 0.1726] [D_B loss: 0.2365] [G loss: 7.3424]\n",
            "[Epoch 4/10] [Batch 107/107] [D_A loss: 0.2295] [D_B loss: 0.2764] [G loss: 4.8585]\n",
            "[Epoch 5/10] [Batch 1/107] [D_A loss: 0.2797] [D_B loss: 0.0838] [G loss: 4.4678]\n",
            "[Epoch 5/10] [Batch 2/107] [D_A loss: 0.2430] [D_B loss: 0.2311] [G loss: 5.3723]\n",
            "[Epoch 5/10] [Batch 3/107] [D_A loss: 0.2398] [D_B loss: 0.1388] [G loss: 11.2698]\n",
            "[Epoch 5/10] [Batch 4/107] [D_A loss: 0.1425] [D_B loss: 0.3841] [G loss: 6.7640]\n",
            "[Epoch 5/10] [Batch 5/107] [D_A loss: 0.1662] [D_B loss: 0.1523] [G loss: 4.5122]\n",
            "[Epoch 5/10] [Batch 6/107] [D_A loss: 0.1885] [D_B loss: 0.2032] [G loss: 6.0216]\n",
            "[Epoch 5/10] [Batch 7/107] [D_A loss: 0.0652] [D_B loss: 0.1970] [G loss: 6.8328]\n",
            "[Epoch 5/10] [Batch 8/107] [D_A loss: 0.1017] [D_B loss: 0.1855] [G loss: 5.4814]\n",
            "[Epoch 5/10] [Batch 9/107] [D_A loss: 0.2301] [D_B loss: 0.1001] [G loss: 6.6258]\n",
            "[Epoch 5/10] [Batch 10/107] [D_A loss: 0.0894] [D_B loss: 0.1136] [G loss: 6.1996]\n",
            "[Epoch 5/10] [Batch 11/107] [D_A loss: 0.2670] [D_B loss: 0.4000] [G loss: 6.5356]\n",
            "[Epoch 5/10] [Batch 12/107] [D_A loss: 0.2077] [D_B loss: 0.0740] [G loss: 4.9643]\n",
            "[Epoch 5/10] [Batch 13/107] [D_A loss: 0.0948] [D_B loss: 0.1267] [G loss: 6.1035]\n",
            "[Epoch 5/10] [Batch 14/107] [D_A loss: 0.0786] [D_B loss: 0.1343] [G loss: 7.4231]\n",
            "[Epoch 5/10] [Batch 15/107] [D_A loss: 0.1033] [D_B loss: 0.2685] [G loss: 9.4782]\n",
            "[Epoch 5/10] [Batch 16/107] [D_A loss: 0.1151] [D_B loss: 0.3173] [G loss: 6.9468]\n",
            "[Epoch 5/10] [Batch 17/107] [D_A loss: 0.2930] [D_B loss: 0.2211] [G loss: 6.8345]\n",
            "[Epoch 5/10] [Batch 18/107] [D_A loss: 0.1904] [D_B loss: 0.1271] [G loss: 6.0418]\n",
            "[Epoch 5/10] [Batch 19/107] [D_A loss: 0.0653] [D_B loss: 0.1887] [G loss: 6.4452]\n",
            "[Epoch 5/10] [Batch 20/107] [D_A loss: 0.3496] [D_B loss: 0.1048] [G loss: 8.3338]\n",
            "[Epoch 5/10] [Batch 21/107] [D_A loss: 0.0653] [D_B loss: 0.2423] [G loss: 6.4497]\n",
            "[Epoch 5/10] [Batch 22/107] [D_A loss: 0.1232] [D_B loss: 0.1644] [G loss: 6.0450]\n",
            "[Epoch 5/10] [Batch 23/107] [D_A loss: 0.1313] [D_B loss: 0.3022] [G loss: 6.2457]\n",
            "[Epoch 5/10] [Batch 24/107] [D_A loss: 0.2143] [D_B loss: 0.1581] [G loss: 6.2553]\n",
            "[Epoch 5/10] [Batch 25/107] [D_A loss: 0.1382] [D_B loss: 0.2156] [G loss: 5.7129]\n",
            "[Epoch 5/10] [Batch 26/107] [D_A loss: 0.1220] [D_B loss: 0.3552] [G loss: 5.8562]\n",
            "[Epoch 5/10] [Batch 27/107] [D_A loss: 0.1582] [D_B loss: 0.2692] [G loss: 4.6942]\n",
            "[Epoch 5/10] [Batch 28/107] [D_A loss: 0.1650] [D_B loss: 0.1189] [G loss: 4.8691]\n",
            "[Epoch 5/10] [Batch 29/107] [D_A loss: 0.0560] [D_B loss: 0.3423] [G loss: 4.6015]\n",
            "[Epoch 5/10] [Batch 30/107] [D_A loss: 0.3406] [D_B loss: 0.0903] [G loss: 5.5288]\n",
            "[Epoch 5/10] [Batch 31/107] [D_A loss: 0.1232] [D_B loss: 0.1898] [G loss: 5.4727]\n",
            "[Epoch 5/10] [Batch 32/107] [D_A loss: 0.3625] [D_B loss: 0.2544] [G loss: 6.9279]\n",
            "[Epoch 5/10] [Batch 33/107] [D_A loss: 0.3192] [D_B loss: 0.2870] [G loss: 4.8588]\n",
            "[Epoch 5/10] [Batch 34/107] [D_A loss: 0.1771] [D_B loss: 0.2891] [G loss: 5.9714]\n",
            "[Epoch 5/10] [Batch 35/107] [D_A loss: 0.1217] [D_B loss: 0.2670] [G loss: 4.7314]\n",
            "[Epoch 5/10] [Batch 36/107] [D_A loss: 0.2257] [D_B loss: 0.2146] [G loss: 5.2420]\n",
            "[Epoch 5/10] [Batch 37/107] [D_A loss: 0.1706] [D_B loss: 0.2667] [G loss: 4.5368]\n",
            "[Epoch 5/10] [Batch 38/107] [D_A loss: 0.0959] [D_B loss: 0.1951] [G loss: 6.8199]\n",
            "[Epoch 5/10] [Batch 39/107] [D_A loss: 0.2920] [D_B loss: 0.1720] [G loss: 4.2834]\n",
            "[Epoch 5/10] [Batch 40/107] [D_A loss: 0.1239] [D_B loss: 0.3555] [G loss: 3.8172]\n",
            "[Epoch 5/10] [Batch 41/107] [D_A loss: 0.3521] [D_B loss: 0.2048] [G loss: 5.4124]\n",
            "[Epoch 5/10] [Batch 42/107] [D_A loss: 0.2374] [D_B loss: 0.3216] [G loss: 4.7468]\n",
            "[Epoch 5/10] [Batch 43/107] [D_A loss: 0.2061] [D_B loss: 0.2973] [G loss: 5.5642]\n",
            "[Epoch 5/10] [Batch 44/107] [D_A loss: 0.1893] [D_B loss: 0.0645] [G loss: 6.9693]\n",
            "[Epoch 5/10] [Batch 45/107] [D_A loss: 0.2065] [D_B loss: 0.1284] [G loss: 5.6281]\n",
            "[Epoch 5/10] [Batch 46/107] [D_A loss: 0.2876] [D_B loss: 0.3663] [G loss: 4.8382]\n",
            "[Epoch 5/10] [Batch 47/107] [D_A loss: 0.1681] [D_B loss: 0.0944] [G loss: 5.3484]\n",
            "[Epoch 5/10] [Batch 48/107] [D_A loss: 0.1423] [D_B loss: 0.3974] [G loss: 4.5659]\n",
            "[Epoch 5/10] [Batch 49/107] [D_A loss: 0.2723] [D_B loss: 0.2954] [G loss: 5.3378]\n",
            "[Epoch 5/10] [Batch 50/107] [D_A loss: 0.1380] [D_B loss: 0.1944] [G loss: 4.4371]\n",
            "[Epoch 5/10] [Batch 51/107] [D_A loss: 0.1860] [D_B loss: 0.1980] [G loss: 5.4512]\n",
            "[Epoch 5/10] [Batch 52/107] [D_A loss: 0.1623] [D_B loss: 0.1724] [G loss: 5.1702]\n",
            "[Epoch 5/10] [Batch 53/107] [D_A loss: 0.1934] [D_B loss: 0.2903] [G loss: 5.5456]\n",
            "[Epoch 5/10] [Batch 54/107] [D_A loss: 0.0880] [D_B loss: 0.3201] [G loss: 5.4658]\n",
            "[Epoch 5/10] [Batch 55/107] [D_A loss: 0.0656] [D_B loss: 0.2324] [G loss: 5.0346]\n",
            "[Epoch 5/10] [Batch 56/107] [D_A loss: 0.1340] [D_B loss: 0.2153] [G loss: 7.2714]\n",
            "[Epoch 5/10] [Batch 57/107] [D_A loss: 0.1662] [D_B loss: 0.2447] [G loss: 5.2458]\n",
            "[Epoch 5/10] [Batch 58/107] [D_A loss: 0.1218] [D_B loss: 0.1077] [G loss: 5.6359]\n",
            "[Epoch 5/10] [Batch 59/107] [D_A loss: 0.2309] [D_B loss: 0.1882] [G loss: 5.5754]\n",
            "[Epoch 5/10] [Batch 60/107] [D_A loss: 0.1018] [D_B loss: 0.1829] [G loss: 5.4091]\n",
            "[Epoch 5/10] [Batch 61/107] [D_A loss: 0.9047] [D_B loss: 0.1587] [G loss: 5.9379]\n",
            "[Epoch 5/10] [Batch 62/107] [D_A loss: 0.2845] [D_B loss: 0.1234] [G loss: 7.9466]\n",
            "[Epoch 5/10] [Batch 63/107] [D_A loss: 0.2089] [D_B loss: 0.1705] [G loss: 5.2007]\n",
            "[Epoch 5/10] [Batch 64/107] [D_A loss: 0.2179] [D_B loss: 0.2189] [G loss: 4.9437]\n",
            "[Epoch 5/10] [Batch 65/107] [D_A loss: 0.2846] [D_B loss: 0.1269] [G loss: 5.4069]\n",
            "[Epoch 5/10] [Batch 66/107] [D_A loss: 0.2194] [D_B loss: 0.0718] [G loss: 5.1808]\n",
            "[Epoch 5/10] [Batch 67/107] [D_A loss: 0.2008] [D_B loss: 0.1190] [G loss: 5.7919]\n",
            "[Epoch 5/10] [Batch 68/107] [D_A loss: 0.2385] [D_B loss: 0.1316] [G loss: 4.4510]\n",
            "[Epoch 5/10] [Batch 69/107] [D_A loss: 0.1437] [D_B loss: 0.1675] [G loss: 4.0974]\n",
            "[Epoch 5/10] [Batch 70/107] [D_A loss: 0.0854] [D_B loss: 0.3247] [G loss: 5.9203]\n",
            "[Epoch 5/10] [Batch 71/107] [D_A loss: 0.3730] [D_B loss: 0.1124] [G loss: 4.5401]\n",
            "[Epoch 5/10] [Batch 72/107] [D_A loss: 0.2151] [D_B loss: 0.2632] [G loss: 4.3557]\n",
            "[Epoch 5/10] [Batch 73/107] [D_A loss: 0.1494] [D_B loss: 0.1976] [G loss: 6.4318]\n",
            "[Epoch 5/10] [Batch 74/107] [D_A loss: 0.1182] [D_B loss: 0.1565] [G loss: 4.6901]\n",
            "[Epoch 5/10] [Batch 75/107] [D_A loss: 0.2389] [D_B loss: 0.0787] [G loss: 5.0681]\n",
            "[Epoch 5/10] [Batch 76/107] [D_A loss: 0.3307] [D_B loss: 0.1268] [G loss: 6.2717]\n",
            "[Epoch 5/10] [Batch 77/107] [D_A loss: 0.1384] [D_B loss: 0.1367] [G loss: 5.8204]\n",
            "[Epoch 5/10] [Batch 78/107] [D_A loss: 0.1909] [D_B loss: 0.1595] [G loss: 5.8312]\n",
            "[Epoch 5/10] [Batch 79/107] [D_A loss: 0.1889] [D_B loss: 0.0700] [G loss: 4.8534]\n",
            "[Epoch 5/10] [Batch 80/107] [D_A loss: 0.1246] [D_B loss: 0.3236] [G loss: 9.8567]\n",
            "[Epoch 5/10] [Batch 81/107] [D_A loss: 0.1598] [D_B loss: 0.2536] [G loss: 5.1354]\n",
            "[Epoch 5/10] [Batch 82/107] [D_A loss: 0.1981] [D_B loss: 0.1211] [G loss: 4.0488]\n",
            "[Epoch 5/10] [Batch 83/107] [D_A loss: 0.1425] [D_B loss: 0.2002] [G loss: 5.1566]\n",
            "[Epoch 5/10] [Batch 84/107] [D_A loss: 0.2108] [D_B loss: 0.1665] [G loss: 3.9694]\n",
            "[Epoch 5/10] [Batch 85/107] [D_A loss: 0.2077] [D_B loss: 0.1231] [G loss: 5.6847]\n",
            "[Epoch 5/10] [Batch 86/107] [D_A loss: 0.2095] [D_B loss: 0.0833] [G loss: 4.5746]\n",
            "[Epoch 5/10] [Batch 87/107] [D_A loss: 0.1861] [D_B loss: 0.3082] [G loss: 5.3994]\n",
            "[Epoch 5/10] [Batch 88/107] [D_A loss: 0.1742] [D_B loss: 0.0668] [G loss: 6.4321]\n",
            "[Epoch 5/10] [Batch 89/107] [D_A loss: 0.1606] [D_B loss: 0.0936] [G loss: 5.6513]\n",
            "[Epoch 5/10] [Batch 90/107] [D_A loss: 0.2032] [D_B loss: 0.2265] [G loss: 5.0471]\n",
            "[Epoch 5/10] [Batch 91/107] [D_A loss: 0.1104] [D_B loss: 0.2618] [G loss: 4.2508]\n",
            "[Epoch 5/10] [Batch 92/107] [D_A loss: 0.1922] [D_B loss: 0.3226] [G loss: 5.5125]\n",
            "[Epoch 5/10] [Batch 93/107] [D_A loss: 0.1269] [D_B loss: 0.4762] [G loss: 5.0728]\n",
            "[Epoch 5/10] [Batch 94/107] [D_A loss: 0.1612] [D_B loss: 0.5200] [G loss: 5.8182]\n",
            "[Epoch 5/10] [Batch 95/107] [D_A loss: 0.1369] [D_B loss: 0.3878] [G loss: 6.9670]\n",
            "[Epoch 5/10] [Batch 96/107] [D_A loss: 0.1337] [D_B loss: 0.1637] [G loss: 7.0840]\n",
            "[Epoch 5/10] [Batch 97/107] [D_A loss: 0.1112] [D_B loss: 0.4604] [G loss: 6.1624]\n",
            "[Epoch 5/10] [Batch 98/107] [D_A loss: 0.2876] [D_B loss: 0.2635] [G loss: 7.3516]\n",
            "[Epoch 5/10] [Batch 99/107] [D_A loss: 0.2907] [D_B loss: 0.1107] [G loss: 4.3324]\n",
            "[Epoch 5/10] [Batch 100/107] [D_A loss: 0.2545] [D_B loss: 0.3002] [G loss: 6.1666]\n",
            "[Epoch 5/10] [Batch 101/107] [D_A loss: 0.3328] [D_B loss: 0.2604] [G loss: 7.4871]\n",
            "[Epoch 5/10] [Batch 102/107] [D_A loss: 0.2149] [D_B loss: 0.1736] [G loss: 4.8259]\n",
            "[Epoch 5/10] [Batch 103/107] [D_A loss: 0.1403] [D_B loss: 0.1548] [G loss: 4.4030]\n",
            "[Epoch 5/10] [Batch 104/107] [D_A loss: 0.2211] [D_B loss: 0.2686] [G loss: 7.9752]\n",
            "[Epoch 5/10] [Batch 105/107] [D_A loss: 0.2142] [D_B loss: 0.2464] [G loss: 5.6123]\n",
            "[Epoch 5/10] [Batch 106/107] [D_A loss: 0.3251] [D_B loss: 0.1338] [G loss: 8.2409]\n",
            "[Epoch 5/10] [Batch 107/107] [D_A loss: 0.2101] [D_B loss: 0.2100] [G loss: 4.5138]\n",
            "[Epoch 6/10] [Batch 1/107] [D_A loss: 0.0853] [D_B loss: 0.1940] [G loss: 5.5800]\n",
            "[Epoch 6/10] [Batch 2/107] [D_A loss: 0.2095] [D_B loss: 0.1039] [G loss: 6.7109]\n",
            "[Epoch 6/10] [Batch 3/107] [D_A loss: 0.0744] [D_B loss: 0.1823] [G loss: 5.9505]\n",
            "[Epoch 6/10] [Batch 4/107] [D_A loss: 0.2606] [D_B loss: 0.2173] [G loss: 4.4370]\n",
            "[Epoch 6/10] [Batch 5/107] [D_A loss: 0.1498] [D_B loss: 0.1022] [G loss: 6.2571]\n",
            "[Epoch 6/10] [Batch 6/107] [D_A loss: 0.1312] [D_B loss: 0.1424] [G loss: 7.4129]\n",
            "[Epoch 6/10] [Batch 7/107] [D_A loss: 0.2040] [D_B loss: 0.1553] [G loss: 7.6088]\n",
            "[Epoch 6/10] [Batch 8/107] [D_A loss: 0.1458] [D_B loss: 0.1723] [G loss: 4.4578]\n",
            "[Epoch 6/10] [Batch 9/107] [D_A loss: 0.0526] [D_B loss: 0.2064] [G loss: 5.3590]\n",
            "[Epoch 6/10] [Batch 10/107] [D_A loss: 0.3001] [D_B loss: 0.2356] [G loss: 5.2924]\n",
            "[Epoch 6/10] [Batch 11/107] [D_A loss: 0.2570] [D_B loss: 0.0907] [G loss: 6.3656]\n",
            "[Epoch 6/10] [Batch 12/107] [D_A loss: 0.3649] [D_B loss: 0.0616] [G loss: 5.3337]\n",
            "[Epoch 6/10] [Batch 13/107] [D_A loss: 0.5052] [D_B loss: 0.1666] [G loss: 5.9117]\n",
            "[Epoch 6/10] [Batch 14/107] [D_A loss: 0.2770] [D_B loss: 0.1351] [G loss: 6.2194]\n",
            "[Epoch 6/10] [Batch 15/107] [D_A loss: 0.2727] [D_B loss: 0.1056] [G loss: 4.6067]\n",
            "[Epoch 6/10] [Batch 16/107] [D_A loss: 0.3038] [D_B loss: 0.3844] [G loss: 6.1190]\n",
            "[Epoch 6/10] [Batch 17/107] [D_A loss: 0.1197] [D_B loss: 0.4295] [G loss: 5.4666]\n",
            "[Epoch 6/10] [Batch 18/107] [D_A loss: 0.3857] [D_B loss: 0.1698] [G loss: 12.4608]\n",
            "[Epoch 6/10] [Batch 19/107] [D_A loss: 0.2385] [D_B loss: 0.1577] [G loss: 5.6034]\n",
            "[Epoch 6/10] [Batch 20/107] [D_A loss: 0.2890] [D_B loss: 0.1893] [G loss: 3.8709]\n",
            "[Epoch 6/10] [Batch 21/107] [D_A loss: 0.1689] [D_B loss: 0.2459] [G loss: 5.0020]\n",
            "[Epoch 6/10] [Batch 22/107] [D_A loss: 0.2121] [D_B loss: 0.0556] [G loss: 5.8942]\n",
            "[Epoch 6/10] [Batch 23/107] [D_A loss: 0.1899] [D_B loss: 0.1689] [G loss: 7.4920]\n",
            "[Epoch 6/10] [Batch 24/107] [D_A loss: 0.1300] [D_B loss: 0.3123] [G loss: 4.1069]\n",
            "[Epoch 6/10] [Batch 25/107] [D_A loss: 0.1748] [D_B loss: 0.1195] [G loss: 4.7707]\n",
            "[Epoch 6/10] [Batch 26/107] [D_A loss: 0.0950] [D_B loss: 0.2341] [G loss: 5.0614]\n",
            "[Epoch 6/10] [Batch 27/107] [D_A loss: 0.2552] [D_B loss: 0.3582] [G loss: 5.2333]\n",
            "[Epoch 6/10] [Batch 28/107] [D_A loss: 0.1465] [D_B loss: 0.1250] [G loss: 9.3505]\n",
            "[Epoch 6/10] [Batch 29/107] [D_A loss: 0.3370] [D_B loss: 0.3010] [G loss: 5.3562]\n",
            "[Epoch 6/10] [Batch 30/107] [D_A loss: 0.2186] [D_B loss: 0.0772] [G loss: 7.5721]\n",
            "[Epoch 6/10] [Batch 31/107] [D_A loss: 0.2242] [D_B loss: 0.1820] [G loss: 7.4585]\n",
            "[Epoch 6/10] [Batch 32/107] [D_A loss: 0.2661] [D_B loss: 0.2965] [G loss: 4.4893]\n",
            "[Epoch 6/10] [Batch 33/107] [D_A loss: 0.3143] [D_B loss: 0.2586] [G loss: 6.9907]\n",
            "[Epoch 6/10] [Batch 34/107] [D_A loss: 0.2279] [D_B loss: 0.1064] [G loss: 4.6909]\n",
            "[Epoch 6/10] [Batch 35/107] [D_A loss: 0.2130] [D_B loss: 0.2369] [G loss: 4.7922]\n",
            "[Epoch 6/10] [Batch 36/107] [D_A loss: 0.1377] [D_B loss: 0.2177] [G loss: 5.2748]\n",
            "[Epoch 6/10] [Batch 37/107] [D_A loss: 0.1933] [D_B loss: 0.2345] [G loss: 5.5055]\n",
            "[Epoch 6/10] [Batch 38/107] [D_A loss: 0.2324] [D_B loss: 0.1514] [G loss: 4.5280]\n",
            "[Epoch 6/10] [Batch 39/107] [D_A loss: 0.1148] [D_B loss: 0.1726] [G loss: 4.7161]\n",
            "[Epoch 6/10] [Batch 40/107] [D_A loss: 0.0532] [D_B loss: 0.1784] [G loss: 4.2573]\n",
            "[Epoch 6/10] [Batch 41/107] [D_A loss: 0.2431] [D_B loss: 0.2110] [G loss: 3.9971]\n",
            "[Epoch 6/10] [Batch 42/107] [D_A loss: 0.2262] [D_B loss: 0.1397] [G loss: 3.9309]\n",
            "[Epoch 6/10] [Batch 43/107] [D_A loss: 0.1532] [D_B loss: 0.3201] [G loss: 4.2173]\n",
            "[Epoch 6/10] [Batch 44/107] [D_A loss: 0.1608] [D_B loss: 0.1682] [G loss: 4.3615]\n",
            "[Epoch 6/10] [Batch 45/107] [D_A loss: 0.1981] [D_B loss: 0.1810] [G loss: 6.7163]\n",
            "[Epoch 6/10] [Batch 46/107] [D_A loss: 0.1468] [D_B loss: 0.2970] [G loss: 5.0217]\n",
            "[Epoch 6/10] [Batch 47/107] [D_A loss: 0.1766] [D_B loss: 0.1674] [G loss: 4.5591]\n",
            "[Epoch 6/10] [Batch 48/107] [D_A loss: 0.1447] [D_B loss: 0.1596] [G loss: 5.4145]\n",
            "[Epoch 6/10] [Batch 49/107] [D_A loss: 0.3008] [D_B loss: 0.0805] [G loss: 7.0317]\n",
            "[Epoch 6/10] [Batch 50/107] [D_A loss: 0.2193] [D_B loss: 0.2225] [G loss: 4.3884]\n",
            "[Epoch 6/10] [Batch 51/107] [D_A loss: 0.2248] [D_B loss: 0.2958] [G loss: 4.6499]\n",
            "[Epoch 6/10] [Batch 52/107] [D_A loss: 0.1858] [D_B loss: 0.2096] [G loss: 4.2270]\n",
            "[Epoch 6/10] [Batch 53/107] [D_A loss: 0.1787] [D_B loss: 0.3019] [G loss: 4.0463]\n",
            "[Epoch 6/10] [Batch 54/107] [D_A loss: 0.1564] [D_B loss: 0.1836] [G loss: 5.8562]\n",
            "[Epoch 6/10] [Batch 55/107] [D_A loss: 0.2274] [D_B loss: 0.2091] [G loss: 4.6082]\n",
            "[Epoch 6/10] [Batch 56/107] [D_A loss: 0.1404] [D_B loss: 0.3663] [G loss: 6.9031]\n",
            "[Epoch 6/10] [Batch 57/107] [D_A loss: 0.2004] [D_B loss: 0.2346] [G loss: 3.6166]\n",
            "[Epoch 6/10] [Batch 58/107] [D_A loss: 0.1473] [D_B loss: 0.0982] [G loss: 6.3836]\n",
            "[Epoch 6/10] [Batch 59/107] [D_A loss: 0.1750] [D_B loss: 0.1964] [G loss: 5.0198]\n",
            "[Epoch 6/10] [Batch 60/107] [D_A loss: 0.2078] [D_B loss: 0.3049] [G loss: 6.3086]\n",
            "[Epoch 6/10] [Batch 61/107] [D_A loss: 0.1524] [D_B loss: 0.3212] [G loss: 4.3529]\n",
            "[Epoch 6/10] [Batch 62/107] [D_A loss: 0.2836] [D_B loss: 0.2250] [G loss: 5.7024]\n",
            "[Epoch 6/10] [Batch 63/107] [D_A loss: 0.3399] [D_B loss: 0.3244] [G loss: 7.3590]\n",
            "[Epoch 6/10] [Batch 64/107] [D_A loss: 0.1042] [D_B loss: 0.2459] [G loss: 4.3011]\n",
            "[Epoch 6/10] [Batch 65/107] [D_A loss: 0.1224] [D_B loss: 0.2671] [G loss: 7.5243]\n",
            "[Epoch 6/10] [Batch 66/107] [D_A loss: 0.1530] [D_B loss: 0.2186] [G loss: 6.0798]\n",
            "[Epoch 6/10] [Batch 67/107] [D_A loss: 0.2231] [D_B loss: 0.2789] [G loss: 5.2958]\n",
            "[Epoch 6/10] [Batch 68/107] [D_A loss: 0.3339] [D_B loss: 0.3427] [G loss: 4.7243]\n",
            "[Epoch 6/10] [Batch 69/107] [D_A loss: 0.2036] [D_B loss: 0.4119] [G loss: 5.3180]\n",
            "[Epoch 6/10] [Batch 70/107] [D_A loss: 0.4282] [D_B loss: 0.2016] [G loss: 6.7066]\n",
            "[Epoch 6/10] [Batch 71/107] [D_A loss: 0.2773] [D_B loss: 0.2027] [G loss: 3.6446]\n",
            "[Epoch 6/10] [Batch 72/107] [D_A loss: 0.3573] [D_B loss: 0.2565] [G loss: 4.7315]\n",
            "[Epoch 6/10] [Batch 73/107] [D_A loss: 0.5115] [D_B loss: 0.3543] [G loss: 7.1337]\n",
            "[Epoch 6/10] [Batch 74/107] [D_A loss: 0.2519] [D_B loss: 0.2497] [G loss: 5.1194]\n",
            "[Epoch 6/10] [Batch 75/107] [D_A loss: 0.2238] [D_B loss: 0.3047] [G loss: 3.4649]\n",
            "[Epoch 6/10] [Batch 76/107] [D_A loss: 0.1936] [D_B loss: 0.1618] [G loss: 5.8331]\n",
            "[Epoch 6/10] [Batch 77/107] [D_A loss: 0.2772] [D_B loss: 0.3198] [G loss: 5.0922]\n",
            "[Epoch 6/10] [Batch 78/107] [D_A loss: 0.2062] [D_B loss: 0.2526] [G loss: 4.6896]\n",
            "[Epoch 6/10] [Batch 79/107] [D_A loss: 0.2286] [D_B loss: 0.1811] [G loss: 5.1880]\n",
            "[Epoch 6/10] [Batch 80/107] [D_A loss: 0.1863] [D_B loss: 0.1769] [G loss: 3.9789]\n",
            "[Epoch 6/10] [Batch 81/107] [D_A loss: 0.1762] [D_B loss: 0.1532] [G loss: 5.3657]\n",
            "[Epoch 6/10] [Batch 82/107] [D_A loss: 0.2605] [D_B loss: 0.2572] [G loss: 7.1023]\n",
            "[Epoch 6/10] [Batch 83/107] [D_A loss: 0.1265] [D_B loss: 0.1253] [G loss: 4.7023]\n",
            "[Epoch 6/10] [Batch 84/107] [D_A loss: 0.1331] [D_B loss: 0.1662] [G loss: 4.7579]\n",
            "[Epoch 6/10] [Batch 85/107] [D_A loss: 0.0877] [D_B loss: 0.0936] [G loss: 6.0809]\n",
            "[Epoch 6/10] [Batch 86/107] [D_A loss: 0.1905] [D_B loss: 0.1485] [G loss: 5.3515]\n",
            "[Epoch 6/10] [Batch 87/107] [D_A loss: 0.2108] [D_B loss: 0.1685] [G loss: 4.4270]\n",
            "[Epoch 6/10] [Batch 88/107] [D_A loss: 0.1552] [D_B loss: 0.2334] [G loss: 3.7602]\n",
            "[Epoch 6/10] [Batch 89/107] [D_A loss: 0.2112] [D_B loss: 0.1220] [G loss: 6.6244]\n",
            "[Epoch 6/10] [Batch 90/107] [D_A loss: 0.1054] [D_B loss: 0.0832] [G loss: 4.4060]\n",
            "[Epoch 6/10] [Batch 91/107] [D_A loss: 0.1002] [D_B loss: 0.2103] [G loss: 4.4078]\n",
            "[Epoch 6/10] [Batch 92/107] [D_A loss: 0.0949] [D_B loss: 0.4338] [G loss: 6.3622]\n",
            "[Epoch 6/10] [Batch 93/107] [D_A loss: 0.0615] [D_B loss: 0.2557] [G loss: 4.8841]\n",
            "[Epoch 6/10] [Batch 94/107] [D_A loss: 0.1234] [D_B loss: 0.2093] [G loss: 4.3219]\n",
            "[Epoch 6/10] [Batch 95/107] [D_A loss: 0.2120] [D_B loss: 0.3118] [G loss: 4.4604]\n",
            "[Epoch 6/10] [Batch 96/107] [D_A loss: 0.1870] [D_B loss: 0.2035] [G loss: 5.1953]\n",
            "[Epoch 6/10] [Batch 97/107] [D_A loss: 0.3464] [D_B loss: 0.1502] [G loss: 9.8910]\n",
            "[Epoch 6/10] [Batch 98/107] [D_A loss: 0.1928] [D_B loss: 0.1228] [G loss: 4.0167]\n",
            "[Epoch 6/10] [Batch 99/107] [D_A loss: 0.2440] [D_B loss: 0.1950] [G loss: 5.6018]\n",
            "[Epoch 6/10] [Batch 100/107] [D_A loss: 0.2437] [D_B loss: 0.1945] [G loss: 4.3008]\n",
            "[Epoch 6/10] [Batch 101/107] [D_A loss: 0.2458] [D_B loss: 0.3433] [G loss: 6.3072]\n",
            "[Epoch 6/10] [Batch 102/107] [D_A loss: 0.1470] [D_B loss: 0.2439] [G loss: 6.8059]\n",
            "[Epoch 6/10] [Batch 103/107] [D_A loss: 0.0910] [D_B loss: 0.2169] [G loss: 5.8500]\n",
            "[Epoch 6/10] [Batch 104/107] [D_A loss: 0.1859] [D_B loss: 0.2164] [G loss: 7.4355]\n",
            "[Epoch 6/10] [Batch 105/107] [D_A loss: 0.2957] [D_B loss: 0.3152] [G loss: 4.7835]\n",
            "[Epoch 6/10] [Batch 106/107] [D_A loss: 0.2124] [D_B loss: 0.2104] [G loss: 4.6162]\n",
            "[Epoch 6/10] [Batch 107/107] [D_A loss: 0.2226] [D_B loss: 0.2025] [G loss: 3.3898]\n",
            "[Epoch 7/10] [Batch 1/107] [D_A loss: 0.1728] [D_B loss: 0.1422] [G loss: 6.5067]\n",
            "[Epoch 7/10] [Batch 2/107] [D_A loss: 0.1800] [D_B loss: 0.2603] [G loss: 7.6558]\n",
            "[Epoch 7/10] [Batch 3/107] [D_A loss: 0.2462] [D_B loss: 0.1373] [G loss: 5.2253]\n",
            "[Epoch 7/10] [Batch 4/107] [D_A loss: 0.1264] [D_B loss: 0.2391] [G loss: 3.9225]\n",
            "[Epoch 7/10] [Batch 5/107] [D_A loss: 0.1701] [D_B loss: 0.1297] [G loss: 5.3779]\n",
            "[Epoch 7/10] [Batch 6/107] [D_A loss: 0.1766] [D_B loss: 0.1751] [G loss: 3.9602]\n",
            "[Epoch 7/10] [Batch 7/107] [D_A loss: 0.1277] [D_B loss: 0.2648] [G loss: 3.7241]\n",
            "[Epoch 7/10] [Batch 8/107] [D_A loss: 0.1927] [D_B loss: 0.3367] [G loss: 4.7552]\n",
            "[Epoch 7/10] [Batch 9/107] [D_A loss: 0.1932] [D_B loss: 0.1885] [G loss: 3.7542]\n",
            "[Epoch 7/10] [Batch 10/107] [D_A loss: 0.2291] [D_B loss: 0.1936] [G loss: 4.9487]\n",
            "[Epoch 7/10] [Batch 11/107] [D_A loss: 0.2167] [D_B loss: 0.1869] [G loss: 4.5800]\n",
            "[Epoch 7/10] [Batch 12/107] [D_A loss: 0.2299] [D_B loss: 0.2476] [G loss: 5.0475]\n",
            "[Epoch 7/10] [Batch 13/107] [D_A loss: 0.2480] [D_B loss: 0.1788] [G loss: 4.2421]\n",
            "[Epoch 7/10] [Batch 14/107] [D_A loss: 0.1872] [D_B loss: 0.2983] [G loss: 4.3497]\n",
            "[Epoch 7/10] [Batch 15/107] [D_A loss: 0.2568] [D_B loss: 0.1759] [G loss: 6.2982]\n",
            "[Epoch 7/10] [Batch 16/107] [D_A loss: 0.2794] [D_B loss: 0.2364] [G loss: 4.7925]\n",
            "[Epoch 7/10] [Batch 17/107] [D_A loss: 0.1865] [D_B loss: 0.3075] [G loss: 5.2515]\n",
            "[Epoch 7/10] [Batch 18/107] [D_A loss: 0.2190] [D_B loss: 0.1946] [G loss: 4.8197]\n",
            "[Epoch 7/10] [Batch 19/107] [D_A loss: 0.2726] [D_B loss: 0.3579] [G loss: 3.7110]\n",
            "[Epoch 7/10] [Batch 20/107] [D_A loss: 0.1828] [D_B loss: 0.1341] [G loss: 6.3132]\n",
            "[Epoch 7/10] [Batch 21/107] [D_A loss: 0.1518] [D_B loss: 0.1972] [G loss: 3.5721]\n",
            "[Epoch 7/10] [Batch 22/107] [D_A loss: 0.2120] [D_B loss: 0.1076] [G loss: 5.5809]\n",
            "[Epoch 7/10] [Batch 23/107] [D_A loss: 0.3088] [D_B loss: 0.3879] [G loss: 5.4356]\n",
            "[Epoch 7/10] [Batch 24/107] [D_A loss: 0.2201] [D_B loss: 0.1896] [G loss: 4.0786]\n",
            "[Epoch 7/10] [Batch 25/107] [D_A loss: 0.1270] [D_B loss: 0.3824] [G loss: 3.5992]\n",
            "[Epoch 7/10] [Batch 26/107] [D_A loss: 0.1352] [D_B loss: 0.1772] [G loss: 7.1407]\n",
            "[Epoch 7/10] [Batch 27/107] [D_A loss: 0.1815] [D_B loss: 0.1709] [G loss: 3.4429]\n",
            "[Epoch 7/10] [Batch 28/107] [D_A loss: 0.2532] [D_B loss: 0.1528] [G loss: 4.9081]\n",
            "[Epoch 7/10] [Batch 29/107] [D_A loss: 0.1837] [D_B loss: 0.2460] [G loss: 6.2277]\n",
            "[Epoch 7/10] [Batch 30/107] [D_A loss: 0.2469] [D_B loss: 0.3287] [G loss: 6.0430]\n",
            "[Epoch 7/10] [Batch 31/107] [D_A loss: 0.1461] [D_B loss: 0.0815] [G loss: 4.0362]\n",
            "[Epoch 7/10] [Batch 32/107] [D_A loss: 0.1917] [D_B loss: 0.2172] [G loss: 3.7625]\n",
            "[Epoch 7/10] [Batch 33/107] [D_A loss: 0.2353] [D_B loss: 0.1832] [G loss: 5.2752]\n",
            "[Epoch 7/10] [Batch 34/107] [D_A loss: 0.1896] [D_B loss: 0.1729] [G loss: 6.1764]\n",
            "[Epoch 7/10] [Batch 35/107] [D_A loss: 0.1584] [D_B loss: 0.1034] [G loss: 4.7952]\n",
            "[Epoch 7/10] [Batch 36/107] [D_A loss: 0.2104] [D_B loss: 0.2646] [G loss: 7.7050]\n",
            "[Epoch 7/10] [Batch 37/107] [D_A loss: 0.3588] [D_B loss: 0.2436] [G loss: 6.5939]\n",
            "[Epoch 7/10] [Batch 38/107] [D_A loss: 0.1071] [D_B loss: 0.1622] [G loss: 5.8736]\n",
            "[Epoch 7/10] [Batch 39/107] [D_A loss: 0.0946] [D_B loss: 0.0716] [G loss: 5.8879]\n",
            "[Epoch 7/10] [Batch 40/107] [D_A loss: 0.1860] [D_B loss: 0.2725] [G loss: 6.1205]\n",
            "[Epoch 7/10] [Batch 41/107] [D_A loss: 0.2985] [D_B loss: 0.1072] [G loss: 9.7598]\n",
            "[Epoch 7/10] [Batch 42/107] [D_A loss: 0.0891] [D_B loss: 0.0790] [G loss: 5.9988]\n",
            "[Epoch 7/10] [Batch 43/107] [D_A loss: 0.2266] [D_B loss: 0.0829] [G loss: 4.5737]\n",
            "[Epoch 7/10] [Batch 44/107] [D_A loss: 0.2202] [D_B loss: 0.1149] [G loss: 8.1031]\n",
            "[Epoch 7/10] [Batch 45/107] [D_A loss: 0.2480] [D_B loss: 0.1845] [G loss: 4.9267]\n",
            "[Epoch 7/10] [Batch 46/107] [D_A loss: 0.0860] [D_B loss: 0.1640] [G loss: 7.6219]\n",
            "[Epoch 7/10] [Batch 47/107] [D_A loss: 0.2459] [D_B loss: 0.1268] [G loss: 4.7360]\n",
            "[Epoch 7/10] [Batch 48/107] [D_A loss: 0.1388] [D_B loss: 0.0680] [G loss: 4.7701]\n",
            "[Epoch 7/10] [Batch 49/107] [D_A loss: 0.1714] [D_B loss: 0.1684] [G loss: 4.6088]\n",
            "[Epoch 7/10] [Batch 50/107] [D_A loss: 0.2111] [D_B loss: 0.4585] [G loss: 5.6174]\n",
            "[Epoch 7/10] [Batch 51/107] [D_A loss: 0.1758] [D_B loss: 0.2352] [G loss: 5.2882]\n",
            "[Epoch 7/10] [Batch 52/107] [D_A loss: 0.1256] [D_B loss: 0.2693] [G loss: 4.5061]\n",
            "[Epoch 7/10] [Batch 53/107] [D_A loss: 0.0532] [D_B loss: 0.2083] [G loss: 5.4281]\n",
            "[Epoch 7/10] [Batch 54/107] [D_A loss: 0.2188] [D_B loss: 0.1618] [G loss: 6.6432]\n",
            "[Epoch 7/10] [Batch 55/107] [D_A loss: 0.1287] [D_B loss: 0.1412] [G loss: 5.5577]\n",
            "[Epoch 7/10] [Batch 56/107] [D_A loss: 0.1515] [D_B loss: 0.1459] [G loss: 6.2789]\n",
            "[Epoch 7/10] [Batch 57/107] [D_A loss: 0.1457] [D_B loss: 0.1937] [G loss: 6.1236]\n",
            "[Epoch 7/10] [Batch 58/107] [D_A loss: 0.1310] [D_B loss: 0.1793] [G loss: 6.0529]\n",
            "[Epoch 7/10] [Batch 59/107] [D_A loss: 0.3373] [D_B loss: 0.0593] [G loss: 6.0886]\n",
            "[Epoch 7/10] [Batch 60/107] [D_A loss: 0.2225] [D_B loss: 0.1466] [G loss: 5.8001]\n",
            "[Epoch 7/10] [Batch 61/107] [D_A loss: 0.0974] [D_B loss: 0.2763] [G loss: 4.8376]\n",
            "[Epoch 7/10] [Batch 62/107] [D_A loss: 0.2112] [D_B loss: 0.2240] [G loss: 4.9712]\n",
            "[Epoch 7/10] [Batch 63/107] [D_A loss: 0.2325] [D_B loss: 0.1740] [G loss: 4.9706]\n",
            "[Epoch 7/10] [Batch 64/107] [D_A loss: 0.1831] [D_B loss: 0.2764] [G loss: 4.7115]\n",
            "[Epoch 7/10] [Batch 65/107] [D_A loss: 0.1217] [D_B loss: 0.3042] [G loss: 4.5274]\n",
            "[Epoch 7/10] [Batch 66/107] [D_A loss: 0.1729] [D_B loss: 0.1573] [G loss: 5.1805]\n",
            "[Epoch 7/10] [Batch 67/107] [D_A loss: 0.2609] [D_B loss: 0.1747] [G loss: 7.3772]\n",
            "[Epoch 7/10] [Batch 68/107] [D_A loss: 0.3014] [D_B loss: 0.2181] [G loss: 5.4554]\n",
            "[Epoch 7/10] [Batch 69/107] [D_A loss: 0.2688] [D_B loss: 0.1413] [G loss: 5.6128]\n",
            "[Epoch 7/10] [Batch 70/107] [D_A loss: 0.4039] [D_B loss: 0.1264] [G loss: 6.7483]\n",
            "[Epoch 7/10] [Batch 71/107] [D_A loss: 0.1416] [D_B loss: 0.1215] [G loss: 4.3471]\n",
            "[Epoch 7/10] [Batch 72/107] [D_A loss: 0.3352] [D_B loss: 0.1835] [G loss: 5.1225]\n",
            "[Epoch 7/10] [Batch 73/107] [D_A loss: 0.2220] [D_B loss: 0.3208] [G loss: 5.6299]\n",
            "[Epoch 7/10] [Batch 74/107] [D_A loss: 0.0644] [D_B loss: 0.1600] [G loss: 5.8361]\n",
            "[Epoch 7/10] [Batch 75/107] [D_A loss: 0.0496] [D_B loss: 0.4250] [G loss: 6.5650]\n",
            "[Epoch 7/10] [Batch 76/107] [D_A loss: 0.3172] [D_B loss: 0.3481] [G loss: 3.9182]\n",
            "[Epoch 7/10] [Batch 77/107] [D_A loss: 0.1335] [D_B loss: 0.2435] [G loss: 4.6094]\n",
            "[Epoch 7/10] [Batch 78/107] [D_A loss: 0.1010] [D_B loss: 0.1483] [G loss: 4.8808]\n",
            "[Epoch 7/10] [Batch 79/107] [D_A loss: 0.2976] [D_B loss: 0.0839] [G loss: 7.9634]\n",
            "[Epoch 7/10] [Batch 80/107] [D_A loss: 0.4302] [D_B loss: 0.3191] [G loss: 5.5963]\n",
            "[Epoch 7/10] [Batch 81/107] [D_A loss: 0.2225] [D_B loss: 0.2942] [G loss: 4.6663]\n",
            "[Epoch 7/10] [Batch 82/107] [D_A loss: 0.2669] [D_B loss: 0.1375] [G loss: 6.1683]\n",
            "[Epoch 7/10] [Batch 83/107] [D_A loss: 0.1847] [D_B loss: 0.1655] [G loss: 8.2201]\n",
            "[Epoch 7/10] [Batch 84/107] [D_A loss: 0.2250] [D_B loss: 0.0878] [G loss: 5.1889]\n",
            "[Epoch 7/10] [Batch 85/107] [D_A loss: 0.2065] [D_B loss: 0.1238] [G loss: 5.7021]\n",
            "[Epoch 7/10] [Batch 86/107] [D_A loss: 0.2440] [D_B loss: 0.3117] [G loss: 4.2937]\n",
            "[Epoch 7/10] [Batch 87/107] [D_A loss: 0.2472] [D_B loss: 0.1848] [G loss: 5.6873]\n",
            "[Epoch 7/10] [Batch 88/107] [D_A loss: 0.2417] [D_B loss: 0.2199] [G loss: 4.9615]\n",
            "[Epoch 7/10] [Batch 89/107] [D_A loss: 0.1885] [D_B loss: 0.1691] [G loss: 4.1403]\n",
            "[Epoch 7/10] [Batch 90/107] [D_A loss: 0.1891] [D_B loss: 0.2475] [G loss: 4.3374]\n",
            "[Epoch 7/10] [Batch 91/107] [D_A loss: 0.1705] [D_B loss: 0.0852] [G loss: 4.7873]\n",
            "[Epoch 7/10] [Batch 92/107] [D_A loss: 0.2457] [D_B loss: 0.0599] [G loss: 4.5824]\n",
            "[Epoch 7/10] [Batch 93/107] [D_A loss: 0.2142] [D_B loss: 0.2898] [G loss: 5.0023]\n",
            "[Epoch 7/10] [Batch 94/107] [D_A loss: 0.1889] [D_B loss: 0.1362] [G loss: 3.9476]\n",
            "[Epoch 7/10] [Batch 95/107] [D_A loss: 0.1467] [D_B loss: 0.1064] [G loss: 9.5702]\n",
            "[Epoch 7/10] [Batch 96/107] [D_A loss: 0.1635] [D_B loss: 0.0888] [G loss: 6.0450]\n",
            "[Epoch 7/10] [Batch 97/107] [D_A loss: 0.1842] [D_B loss: 0.0625] [G loss: 5.7060]\n",
            "[Epoch 7/10] [Batch 98/107] [D_A loss: 0.3943] [D_B loss: 0.2164] [G loss: 3.7960]\n",
            "[Epoch 7/10] [Batch 99/107] [D_A loss: 0.2819] [D_B loss: 0.1930] [G loss: 4.6799]\n",
            "[Epoch 7/10] [Batch 100/107] [D_A loss: 0.1040] [D_B loss: 0.1907] [G loss: 4.5778]\n",
            "[Epoch 7/10] [Batch 101/107] [D_A loss: 0.3717] [D_B loss: 0.1893] [G loss: 3.7716]\n",
            "[Epoch 7/10] [Batch 102/107] [D_A loss: 0.2149] [D_B loss: 0.1931] [G loss: 3.8820]\n",
            "[Epoch 7/10] [Batch 103/107] [D_A loss: 0.2081] [D_B loss: 0.3408] [G loss: 5.4151]\n",
            "[Epoch 7/10] [Batch 104/107] [D_A loss: 0.2004] [D_B loss: 0.1724] [G loss: 4.2928]\n",
            "[Epoch 7/10] [Batch 105/107] [D_A loss: 0.1703] [D_B loss: 0.0813] [G loss: 6.5803]\n",
            "[Epoch 7/10] [Batch 106/107] [D_A loss: 0.1025] [D_B loss: 0.4611] [G loss: 4.8251]\n",
            "[Epoch 7/10] [Batch 107/107] [D_A loss: 0.2022] [D_B loss: 0.3254] [G loss: 3.8506]\n",
            "[Epoch 8/10] [Batch 1/107] [D_A loss: 0.2164] [D_B loss: 0.3735] [G loss: 4.5262]\n",
            "[Epoch 8/10] [Batch 2/107] [D_A loss: 0.1656] [D_B loss: 0.1630] [G loss: 4.7285]\n",
            "[Epoch 8/10] [Batch 3/107] [D_A loss: 0.1583] [D_B loss: 0.4771] [G loss: 10.6296]\n",
            "[Epoch 8/10] [Batch 4/107] [D_A loss: 0.1677] [D_B loss: 0.1356] [G loss: 5.3192]\n",
            "[Epoch 8/10] [Batch 5/107] [D_A loss: 0.2291] [D_B loss: 0.2817] [G loss: 3.9153]\n",
            "[Epoch 8/10] [Batch 6/107] [D_A loss: 0.1758] [D_B loss: 0.1201] [G loss: 5.5605]\n",
            "[Epoch 8/10] [Batch 7/107] [D_A loss: 0.1621] [D_B loss: 0.1715] [G loss: 5.0616]\n",
            "[Epoch 8/10] [Batch 8/107] [D_A loss: 0.0875] [D_B loss: 0.1616] [G loss: 5.2200]\n",
            "[Epoch 8/10] [Batch 9/107] [D_A loss: 0.2389] [D_B loss: 0.0518] [G loss: 7.3721]\n",
            "[Epoch 8/10] [Batch 10/107] [D_A loss: 0.2692] [D_B loss: 0.2729] [G loss: 4.5462]\n",
            "[Epoch 8/10] [Batch 11/107] [D_A loss: 0.2947] [D_B loss: 0.2985] [G loss: 7.2992]\n",
            "[Epoch 8/10] [Batch 12/107] [D_A loss: 0.1505] [D_B loss: 0.2103] [G loss: 5.6606]\n",
            "[Epoch 8/10] [Batch 13/107] [D_A loss: 0.1016] [D_B loss: 0.2635] [G loss: 4.0316]\n",
            "[Epoch 8/10] [Batch 14/107] [D_A loss: 0.4134] [D_B loss: 0.1853] [G loss: 6.0299]\n",
            "[Epoch 8/10] [Batch 15/107] [D_A loss: 0.2973] [D_B loss: 0.1641] [G loss: 4.6365]\n",
            "[Epoch 8/10] [Batch 16/107] [D_A loss: 0.1696] [D_B loss: 0.2765] [G loss: 5.5577]\n",
            "[Epoch 8/10] [Batch 17/107] [D_A loss: 0.2183] [D_B loss: 0.1452] [G loss: 5.6074]\n",
            "[Epoch 8/10] [Batch 18/107] [D_A loss: 0.2272] [D_B loss: 0.2058] [G loss: 3.8052]\n",
            "[Epoch 8/10] [Batch 19/107] [D_A loss: 0.1311] [D_B loss: 0.2369] [G loss: 4.5618]\n",
            "[Epoch 8/10] [Batch 20/107] [D_A loss: 0.1379] [D_B loss: 0.4075] [G loss: 4.8246]\n",
            "[Epoch 8/10] [Batch 21/107] [D_A loss: 0.2753] [D_B loss: 0.4194] [G loss: 3.4580]\n",
            "[Epoch 8/10] [Batch 22/107] [D_A loss: 0.1942] [D_B loss: 0.1370] [G loss: 4.8289]\n",
            "[Epoch 8/10] [Batch 23/107] [D_A loss: 0.2657] [D_B loss: 0.2986] [G loss: 6.6929]\n",
            "[Epoch 8/10] [Batch 24/107] [D_A loss: 0.0919] [D_B loss: 0.1340] [G loss: 5.1021]\n",
            "[Epoch 8/10] [Batch 25/107] [D_A loss: 0.1692] [D_B loss: 0.1400] [G loss: 10.9472]\n",
            "[Epoch 8/10] [Batch 26/107] [D_A loss: 0.2026] [D_B loss: 0.1567] [G loss: 4.4974]\n",
            "[Epoch 8/10] [Batch 27/107] [D_A loss: 0.1915] [D_B loss: 0.1824] [G loss: 5.4740]\n",
            "[Epoch 8/10] [Batch 28/107] [D_A loss: 0.1114] [D_B loss: 0.2957] [G loss: 7.7667]\n",
            "[Epoch 8/10] [Batch 29/107] [D_A loss: 0.2747] [D_B loss: 0.0753] [G loss: 7.1572]\n",
            "[Epoch 8/10] [Batch 30/107] [D_A loss: 0.1336] [D_B loss: 0.1248] [G loss: 3.7737]\n",
            "[Epoch 8/10] [Batch 31/107] [D_A loss: 0.1196] [D_B loss: 0.2044] [G loss: 4.3807]\n",
            "[Epoch 8/10] [Batch 32/107] [D_A loss: 0.1798] [D_B loss: 0.1582] [G loss: 3.6256]\n",
            "[Epoch 8/10] [Batch 33/107] [D_A loss: 0.1373] [D_B loss: 0.1142] [G loss: 4.7218]\n",
            "[Epoch 8/10] [Batch 34/107] [D_A loss: 0.3239] [D_B loss: 0.1642] [G loss: 4.9352]\n",
            "[Epoch 8/10] [Batch 35/107] [D_A loss: 0.1156] [D_B loss: 0.0836] [G loss: 4.3197]\n",
            "[Epoch 8/10] [Batch 36/107] [D_A loss: 0.1752] [D_B loss: 0.1667] [G loss: 10.8092]\n",
            "[Epoch 8/10] [Batch 37/107] [D_A loss: 0.0761] [D_B loss: 0.2552] [G loss: 6.2717]\n",
            "[Epoch 8/10] [Batch 38/107] [D_A loss: 0.1843] [D_B loss: 0.3866] [G loss: 4.8221]\n",
            "[Epoch 8/10] [Batch 39/107] [D_A loss: 0.0825] [D_B loss: 0.2129] [G loss: 5.3988]\n",
            "[Epoch 8/10] [Batch 40/107] [D_A loss: 0.0975] [D_B loss: 0.0547] [G loss: 4.9444]\n",
            "[Epoch 8/10] [Batch 41/107] [D_A loss: 0.1217] [D_B loss: 0.2108] [G loss: 5.6754]\n",
            "[Epoch 8/10] [Batch 42/107] [D_A loss: 0.3126] [D_B loss: 0.1944] [G loss: 6.9542]\n",
            "[Epoch 8/10] [Batch 43/107] [D_A loss: 0.1448] [D_B loss: 0.3120] [G loss: 4.8646]\n",
            "[Epoch 8/10] [Batch 44/107] [D_A loss: 0.2011] [D_B loss: 0.3218] [G loss: 6.2867]\n",
            "[Epoch 8/10] [Batch 45/107] [D_A loss: 0.1349] [D_B loss: 0.2069] [G loss: 4.9818]\n",
            "[Epoch 8/10] [Batch 46/107] [D_A loss: 0.0944] [D_B loss: 0.2494] [G loss: 3.9423]\n",
            "[Epoch 8/10] [Batch 47/107] [D_A loss: 0.1251] [D_B loss: 0.1874] [G loss: 4.6183]\n",
            "[Epoch 8/10] [Batch 48/107] [D_A loss: 0.0971] [D_B loss: 0.2318] [G loss: 5.5008]\n",
            "[Epoch 8/10] [Batch 49/107] [D_A loss: 0.3152] [D_B loss: 0.4908] [G loss: 5.5421]\n",
            "[Epoch 8/10] [Batch 50/107] [D_A loss: 0.1867] [D_B loss: 0.2272] [G loss: 5.8176]\n",
            "[Epoch 8/10] [Batch 51/107] [D_A loss: 0.1267] [D_B loss: 0.1896] [G loss: 6.2149]\n",
            "[Epoch 8/10] [Batch 52/107] [D_A loss: 0.1500] [D_B loss: 0.2545] [G loss: 4.1202]\n",
            "[Epoch 8/10] [Batch 53/107] [D_A loss: 0.1542] [D_B loss: 0.1767] [G loss: 4.7239]\n",
            "[Epoch 8/10] [Batch 54/107] [D_A loss: 0.2206] [D_B loss: 0.1720] [G loss: 5.4087]\n",
            "[Epoch 8/10] [Batch 55/107] [D_A loss: 0.1974] [D_B loss: 0.1295] [G loss: 3.8798]\n",
            "[Epoch 8/10] [Batch 56/107] [D_A loss: 0.1719] [D_B loss: 0.1437] [G loss: 4.1994]\n",
            "[Epoch 8/10] [Batch 57/107] [D_A loss: 0.1766] [D_B loss: 0.1306] [G loss: 5.8393]\n",
            "[Epoch 8/10] [Batch 58/107] [D_A loss: 0.1334] [D_B loss: 0.2301] [G loss: 4.3627]\n",
            "[Epoch 8/10] [Batch 59/107] [D_A loss: 0.2632] [D_B loss: 0.2760] [G loss: 4.0510]\n",
            "[Epoch 8/10] [Batch 60/107] [D_A loss: 0.1669] [D_B loss: 0.1737] [G loss: 7.4917]\n",
            "[Epoch 8/10] [Batch 61/107] [D_A loss: 0.1141] [D_B loss: 0.2352] [G loss: 4.1260]\n",
            "[Epoch 8/10] [Batch 62/107] [D_A loss: 0.1395] [D_B loss: 0.3406] [G loss: 4.1898]\n",
            "[Epoch 8/10] [Batch 63/107] [D_A loss: 0.1956] [D_B loss: 0.1857] [G loss: 4.0198]\n",
            "[Epoch 8/10] [Batch 64/107] [D_A loss: 0.2763] [D_B loss: 0.0860] [G loss: 4.4526]\n",
            "[Epoch 8/10] [Batch 65/107] [D_A loss: 0.1871] [D_B loss: 0.2085] [G loss: 4.3434]\n",
            "[Epoch 8/10] [Batch 66/107] [D_A loss: 0.1875] [D_B loss: 0.1238] [G loss: 4.2364]\n",
            "[Epoch 8/10] [Batch 67/107] [D_A loss: 0.1089] [D_B loss: 0.0818] [G loss: 3.9006]\n",
            "[Epoch 8/10] [Batch 68/107] [D_A loss: 0.2319] [D_B loss: 0.1136] [G loss: 7.5442]\n",
            "[Epoch 8/10] [Batch 69/107] [D_A loss: 0.2574] [D_B loss: 0.3039] [G loss: 7.0183]\n",
            "[Epoch 8/10] [Batch 70/107] [D_A loss: 0.0825] [D_B loss: 0.1845] [G loss: 5.1932]\n",
            "[Epoch 8/10] [Batch 71/107] [D_A loss: 0.4760] [D_B loss: 0.1950] [G loss: 5.4947]\n",
            "[Epoch 8/10] [Batch 72/107] [D_A loss: 0.2118] [D_B loss: 0.0830] [G loss: 5.9790]\n",
            "[Epoch 8/10] [Batch 73/107] [D_A loss: 0.0915] [D_B loss: 0.4275] [G loss: 4.2950]\n",
            "[Epoch 8/10] [Batch 74/107] [D_A loss: 0.2742] [D_B loss: 0.2143] [G loss: 7.5577]\n",
            "[Epoch 8/10] [Batch 75/107] [D_A loss: 0.2564] [D_B loss: 0.2750] [G loss: 3.9756]\n",
            "[Epoch 8/10] [Batch 76/107] [D_A loss: 0.1920] [D_B loss: 0.2433] [G loss: 5.6226]\n",
            "[Epoch 8/10] [Batch 77/107] [D_A loss: 0.1583] [D_B loss: 0.1349] [G loss: 5.3145]\n",
            "[Epoch 8/10] [Batch 78/107] [D_A loss: 0.1152] [D_B loss: 0.3600] [G loss: 5.5845]\n",
            "[Epoch 8/10] [Batch 79/107] [D_A loss: 0.3483] [D_B loss: 0.2078] [G loss: 3.9607]\n",
            "[Epoch 8/10] [Batch 80/107] [D_A loss: 0.2171] [D_B loss: 0.1620] [G loss: 5.0928]\n",
            "[Epoch 8/10] [Batch 81/107] [D_A loss: 0.1610] [D_B loss: 0.2466] [G loss: 7.3724]\n",
            "[Epoch 8/10] [Batch 82/107] [D_A loss: 0.1874] [D_B loss: 0.4449] [G loss: 4.7672]\n",
            "[Epoch 8/10] [Batch 83/107] [D_A loss: 0.0862] [D_B loss: 0.1887] [G loss: 5.5626]\n",
            "[Epoch 8/10] [Batch 84/107] [D_A loss: 0.2759] [D_B loss: 0.5385] [G loss: 4.6883]\n",
            "[Epoch 8/10] [Batch 85/107] [D_A loss: 0.2201] [D_B loss: 0.1284] [G loss: 5.2041]\n",
            "[Epoch 8/10] [Batch 86/107] [D_A loss: 0.2048] [D_B loss: 0.6700] [G loss: 5.8113]\n",
            "[Epoch 8/10] [Batch 87/107] [D_A loss: 0.0963] [D_B loss: 0.2611] [G loss: 4.3530]\n",
            "[Epoch 8/10] [Batch 88/107] [D_A loss: 0.2430] [D_B loss: 0.2742] [G loss: 6.5875]\n",
            "[Epoch 8/10] [Batch 89/107] [D_A loss: 0.0935] [D_B loss: 0.1041] [G loss: 6.8923]\n",
            "[Epoch 8/10] [Batch 90/107] [D_A loss: 0.0726] [D_B loss: 0.2394] [G loss: 6.2860]\n",
            "[Epoch 8/10] [Batch 91/107] [D_A loss: 0.1388] [D_B loss: 0.3020] [G loss: 4.6954]\n",
            "[Epoch 8/10] [Batch 92/107] [D_A loss: 0.2084] [D_B loss: 0.2032] [G loss: 4.1186]\n",
            "[Epoch 8/10] [Batch 93/107] [D_A loss: 0.3154] [D_B loss: 0.0883] [G loss: 6.4726]\n",
            "[Epoch 8/10] [Batch 94/107] [D_A loss: 0.2305] [D_B loss: 0.3211] [G loss: 4.9510]\n",
            "[Epoch 8/10] [Batch 95/107] [D_A loss: 0.2367] [D_B loss: 0.3230] [G loss: 4.3515]\n",
            "[Epoch 8/10] [Batch 96/107] [D_A loss: 0.4464] [D_B loss: 0.2140] [G loss: 3.8751]\n",
            "[Epoch 8/10] [Batch 97/107] [D_A loss: 0.2308] [D_B loss: 0.1343] [G loss: 6.7107]\n",
            "[Epoch 8/10] [Batch 98/107] [D_A loss: 0.2416] [D_B loss: 0.1369] [G loss: 5.1272]\n",
            "[Epoch 8/10] [Batch 99/107] [D_A loss: 0.2352] [D_B loss: 0.2812] [G loss: 5.0383]\n",
            "[Epoch 8/10] [Batch 100/107] [D_A loss: 0.1731] [D_B loss: 0.2681] [G loss: 6.4763]\n",
            "[Epoch 8/10] [Batch 101/107] [D_A loss: 0.1669] [D_B loss: 0.3452] [G loss: 6.4664]\n",
            "[Epoch 8/10] [Batch 102/107] [D_A loss: 0.1123] [D_B loss: 0.4032] [G loss: 4.5423]\n",
            "[Epoch 8/10] [Batch 103/107] [D_A loss: 0.1631] [D_B loss: 0.1000] [G loss: 3.8890]\n",
            "[Epoch 8/10] [Batch 104/107] [D_A loss: 0.2346] [D_B loss: 0.1121] [G loss: 4.7831]\n",
            "[Epoch 8/10] [Batch 105/107] [D_A loss: 0.0576] [D_B loss: 0.1687] [G loss: 5.7920]\n",
            "[Epoch 8/10] [Batch 106/107] [D_A loss: 0.0955] [D_B loss: 0.3988] [G loss: 5.3164]\n",
            "[Epoch 8/10] [Batch 107/107] [D_A loss: 0.1123] [D_B loss: 0.0835] [G loss: 6.3824]\n",
            "[Epoch 9/10] [Batch 1/107] [D_A loss: 0.1209] [D_B loss: 0.1324] [G loss: 4.5867]\n",
            "[Epoch 9/10] [Batch 2/107] [D_A loss: 0.1428] [D_B loss: 0.2769] [G loss: 7.5114]\n",
            "[Epoch 9/10] [Batch 3/107] [D_A loss: 0.1471] [D_B loss: 0.2106] [G loss: 4.4141]\n",
            "[Epoch 9/10] [Batch 4/107] [D_A loss: 0.1394] [D_B loss: 0.1879] [G loss: 4.8625]\n",
            "[Epoch 9/10] [Batch 5/107] [D_A loss: 0.0983] [D_B loss: 0.1389] [G loss: 6.0231]\n",
            "[Epoch 9/10] [Batch 6/107] [D_A loss: 0.2734] [D_B loss: 0.1306] [G loss: 4.4437]\n",
            "[Epoch 9/10] [Batch 7/107] [D_A loss: 0.0751] [D_B loss: 0.1659] [G loss: 4.5818]\n",
            "[Epoch 9/10] [Batch 8/107] [D_A loss: 0.3948] [D_B loss: 0.2496] [G loss: 4.5083]\n",
            "[Epoch 9/10] [Batch 9/107] [D_A loss: 0.2628] [D_B loss: 0.1674] [G loss: 4.9961]\n",
            "[Epoch 9/10] [Batch 10/107] [D_A loss: 0.1020] [D_B loss: 0.2383] [G loss: 3.9485]\n",
            "[Epoch 9/10] [Batch 11/107] [D_A loss: 0.1967] [D_B loss: 0.1656] [G loss: 4.0001]\n",
            "[Epoch 9/10] [Batch 12/107] [D_A loss: 0.1038] [D_B loss: 0.1976] [G loss: 4.5202]\n",
            "[Epoch 9/10] [Batch 13/107] [D_A loss: 0.1527] [D_B loss: 0.1395] [G loss: 4.3166]\n",
            "[Epoch 9/10] [Batch 14/107] [D_A loss: 0.1722] [D_B loss: 0.0821] [G loss: 5.5031]\n",
            "[Epoch 9/10] [Batch 15/107] [D_A loss: 0.3506] [D_B loss: 0.3163] [G loss: 3.9346]\n",
            "[Epoch 9/10] [Batch 16/107] [D_A loss: 0.3279] [D_B loss: 0.2814] [G loss: 6.0415]\n",
            "[Epoch 9/10] [Batch 17/107] [D_A loss: 0.2110] [D_B loss: 0.2337] [G loss: 3.5568]\n",
            "[Epoch 9/10] [Batch 18/107] [D_A loss: 0.2413] [D_B loss: 0.3088] [G loss: 5.0800]\n",
            "[Epoch 9/10] [Batch 19/107] [D_A loss: 0.2822] [D_B loss: 0.2481] [G loss: 4.9596]\n",
            "[Epoch 9/10] [Batch 20/107] [D_A loss: 0.1689] [D_B loss: 0.1788] [G loss: 3.9651]\n",
            "[Epoch 9/10] [Batch 21/107] [D_A loss: 0.1382] [D_B loss: 0.2002] [G loss: 4.7511]\n",
            "[Epoch 9/10] [Batch 22/107] [D_A loss: 0.1386] [D_B loss: 0.1964] [G loss: 5.8940]\n",
            "[Epoch 9/10] [Batch 23/107] [D_A loss: 0.3059] [D_B loss: 0.3692] [G loss: 4.1079]\n",
            "[Epoch 9/10] [Batch 24/107] [D_A loss: 0.2310] [D_B loss: 0.3008] [G loss: 4.9646]\n",
            "[Epoch 9/10] [Batch 25/107] [D_A loss: 0.1072] [D_B loss: 0.1489] [G loss: 4.3311]\n",
            "[Epoch 9/10] [Batch 26/107] [D_A loss: 0.2234] [D_B loss: 0.2643] [G loss: 4.2626]\n",
            "[Epoch 9/10] [Batch 27/107] [D_A loss: 0.1355] [D_B loss: 0.2261] [G loss: 4.1460]\n",
            "[Epoch 9/10] [Batch 28/107] [D_A loss: 0.1420] [D_B loss: 0.2602] [G loss: 5.3108]\n",
            "[Epoch 9/10] [Batch 29/107] [D_A loss: 0.2717] [D_B loss: 0.2318] [G loss: 3.4822]\n",
            "[Epoch 9/10] [Batch 30/107] [D_A loss: 0.1457] [D_B loss: 0.1105] [G loss: 3.5612]\n",
            "[Epoch 9/10] [Batch 31/107] [D_A loss: 0.0688] [D_B loss: 0.1865] [G loss: 6.4733]\n",
            "[Epoch 9/10] [Batch 32/107] [D_A loss: 0.2251] [D_B loss: 0.1806] [G loss: 4.7962]\n",
            "[Epoch 9/10] [Batch 33/107] [D_A loss: 0.1988] [D_B loss: 0.1532] [G loss: 4.3360]\n",
            "[Epoch 9/10] [Batch 34/107] [D_A loss: 0.0720] [D_B loss: 0.1118] [G loss: 6.8624]\n",
            "[Epoch 9/10] [Batch 35/107] [D_A loss: 0.2517] [D_B loss: 0.3261] [G loss: 4.0088]\n",
            "[Epoch 9/10] [Batch 36/107] [D_A loss: 0.2561] [D_B loss: 0.2423] [G loss: 6.9548]\n",
            "[Epoch 9/10] [Batch 37/107] [D_A loss: 0.0486] [D_B loss: 0.3079] [G loss: 7.0561]\n",
            "[Epoch 9/10] [Batch 38/107] [D_A loss: 0.0766] [D_B loss: 0.1430] [G loss: 7.5982]\n",
            "[Epoch 9/10] [Batch 39/107] [D_A loss: 0.2347] [D_B loss: 0.1769] [G loss: 6.6646]\n",
            "[Epoch 9/10] [Batch 40/107] [D_A loss: 0.0977] [D_B loss: 0.1354] [G loss: 4.5645]\n",
            "[Epoch 9/10] [Batch 41/107] [D_A loss: 0.1731] [D_B loss: 0.3938] [G loss: 6.8824]\n",
            "[Epoch 9/10] [Batch 42/107] [D_A loss: 0.1324] [D_B loss: 0.1123] [G loss: 4.9045]\n",
            "[Epoch 9/10] [Batch 43/107] [D_A loss: 0.3790] [D_B loss: 0.2230] [G loss: 6.6204]\n",
            "[Epoch 9/10] [Batch 44/107] [D_A loss: 0.2230] [D_B loss: 0.3319] [G loss: 4.9404]\n",
            "[Epoch 9/10] [Batch 45/107] [D_A loss: 0.2296] [D_B loss: 0.2866] [G loss: 4.5748]\n",
            "[Epoch 9/10] [Batch 46/107] [D_A loss: 0.1514] [D_B loss: 0.2884] [G loss: 4.9908]\n",
            "[Epoch 9/10] [Batch 47/107] [D_A loss: 0.0625] [D_B loss: 0.1785] [G loss: 5.5491]\n",
            "[Epoch 9/10] [Batch 48/107] [D_A loss: 0.3294] [D_B loss: 0.2895] [G loss: 3.7135]\n",
            "[Epoch 9/10] [Batch 49/107] [D_A loss: 0.2548] [D_B loss: 0.3177] [G loss: 3.0816]\n",
            "[Epoch 9/10] [Batch 50/107] [D_A loss: 0.2453] [D_B loss: 0.1005] [G loss: 4.0718]\n",
            "[Epoch 9/10] [Batch 51/107] [D_A loss: 0.2826] [D_B loss: 0.1947] [G loss: 7.3709]\n",
            "[Epoch 9/10] [Batch 52/107] [D_A loss: 0.1758] [D_B loss: 0.1956] [G loss: 4.5555]\n",
            "[Epoch 9/10] [Batch 53/107] [D_A loss: 0.0760] [D_B loss: 0.1044] [G loss: 6.1579]\n",
            "[Epoch 9/10] [Batch 54/107] [D_A loss: 0.1726] [D_B loss: 0.1514] [G loss: 5.2072]\n",
            "[Epoch 9/10] [Batch 55/107] [D_A loss: 0.1533] [D_B loss: 0.1617] [G loss: 4.0593]\n",
            "[Epoch 9/10] [Batch 56/107] [D_A loss: 0.1899] [D_B loss: 0.1506] [G loss: 3.4774]\n",
            "[Epoch 9/10] [Batch 57/107] [D_A loss: 0.2192] [D_B loss: 0.2795] [G loss: 6.0494]\n",
            "[Epoch 9/10] [Batch 58/107] [D_A loss: 0.0975] [D_B loss: 0.3130] [G loss: 4.8321]\n",
            "[Epoch 9/10] [Batch 59/107] [D_A loss: 0.1812] [D_B loss: 0.1900] [G loss: 11.6067]\n",
            "[Epoch 9/10] [Batch 60/107] [D_A loss: 0.2373] [D_B loss: 0.6989] [G loss: 4.2113]\n",
            "[Epoch 9/10] [Batch 61/107] [D_A loss: 0.2049] [D_B loss: 0.0863] [G loss: 4.7697]\n",
            "[Epoch 9/10] [Batch 62/107] [D_A loss: 0.1304] [D_B loss: 0.1797] [G loss: 4.6802]\n",
            "[Epoch 9/10] [Batch 63/107] [D_A loss: 0.0668] [D_B loss: 0.3056] [G loss: 5.6550]\n",
            "[Epoch 9/10] [Batch 64/107] [D_A loss: 0.0555] [D_B loss: 0.1633] [G loss: 4.3145]\n",
            "[Epoch 9/10] [Batch 65/107] [D_A loss: 0.1205] [D_B loss: 0.0940] [G loss: 5.9197]\n",
            "[Epoch 9/10] [Batch 66/107] [D_A loss: 0.2177] [D_B loss: 0.3587] [G loss: 7.0621]\n",
            "[Epoch 9/10] [Batch 67/107] [D_A loss: 0.1822] [D_B loss: 0.1969] [G loss: 4.6281]\n",
            "[Epoch 9/10] [Batch 68/107] [D_A loss: 0.4420] [D_B loss: 0.1976] [G loss: 8.0434]\n",
            "[Epoch 9/10] [Batch 69/107] [D_A loss: 0.2361] [D_B loss: 0.0956] [G loss: 5.9570]\n",
            "[Epoch 9/10] [Batch 70/107] [D_A loss: 0.0770] [D_B loss: 0.1730] [G loss: 3.6056]\n",
            "[Epoch 9/10] [Batch 71/107] [D_A loss: 0.2199] [D_B loss: 0.2064] [G loss: 4.9265]\n",
            "[Epoch 9/10] [Batch 72/107] [D_A loss: 0.2543] [D_B loss: 0.1409] [G loss: 4.6749]\n",
            "[Epoch 9/10] [Batch 73/107] [D_A loss: 0.1434] [D_B loss: 0.2304] [G loss: 6.8676]\n",
            "[Epoch 9/10] [Batch 74/107] [D_A loss: 0.2922] [D_B loss: 0.0552] [G loss: 3.9918]\n",
            "[Epoch 9/10] [Batch 75/107] [D_A loss: 0.0374] [D_B loss: 0.3296] [G loss: 4.1260]\n",
            "[Epoch 9/10] [Batch 76/107] [D_A loss: 0.2108] [D_B loss: 0.2250] [G loss: 4.6782]\n",
            "[Epoch 9/10] [Batch 77/107] [D_A loss: 0.1535] [D_B loss: 0.0728] [G loss: 5.6027]\n",
            "[Epoch 9/10] [Batch 78/107] [D_A loss: 0.1859] [D_B loss: 0.3067] [G loss: 4.4514]\n",
            "[Epoch 9/10] [Batch 79/107] [D_A loss: 0.1909] [D_B loss: 0.2437] [G loss: 4.3362]\n",
            "[Epoch 9/10] [Batch 80/107] [D_A loss: 0.1191] [D_B loss: 0.1559] [G loss: 5.1526]\n",
            "[Epoch 9/10] [Batch 81/107] [D_A loss: 0.2797] [D_B loss: 0.1741] [G loss: 9.3990]\n",
            "[Epoch 9/10] [Batch 82/107] [D_A loss: 0.1681] [D_B loss: 0.3425] [G loss: 5.7661]\n",
            "[Epoch 9/10] [Batch 83/107] [D_A loss: 0.3374] [D_B loss: 0.3077] [G loss: 4.7490]\n",
            "[Epoch 9/10] [Batch 84/107] [D_A loss: 0.2105] [D_B loss: 0.2272] [G loss: 6.2432]\n",
            "[Epoch 9/10] [Batch 85/107] [D_A loss: 0.1791] [D_B loss: 0.1007] [G loss: 4.2053]\n",
            "[Epoch 9/10] [Batch 86/107] [D_A loss: 0.0831] [D_B loss: 0.3726] [G loss: 3.8757]\n",
            "[Epoch 9/10] [Batch 87/107] [D_A loss: 0.2088] [D_B loss: 0.1724] [G loss: 4.2737]\n",
            "[Epoch 9/10] [Batch 88/107] [D_A loss: 0.2349] [D_B loss: 0.3274] [G loss: 5.0062]\n",
            "[Epoch 9/10] [Batch 89/107] [D_A loss: 0.1040] [D_B loss: 0.2617] [G loss: 4.8322]\n",
            "[Epoch 9/10] [Batch 90/107] [D_A loss: 0.2562] [D_B loss: 0.0940] [G loss: 4.5620]\n",
            "[Epoch 9/10] [Batch 91/107] [D_A loss: 0.2464] [D_B loss: 0.1213] [G loss: 5.2431]\n",
            "[Epoch 9/10] [Batch 92/107] [D_A loss: 0.1386] [D_B loss: 0.2537] [G loss: 6.0253]\n",
            "[Epoch 9/10] [Batch 93/107] [D_A loss: 0.1438] [D_B loss: 0.1921] [G loss: 5.1912]\n",
            "[Epoch 9/10] [Batch 94/107] [D_A loss: 0.1442] [D_B loss: 0.5489] [G loss: 5.7198]\n",
            "[Epoch 9/10] [Batch 95/107] [D_A loss: 0.2033] [D_B loss: 0.2241] [G loss: 4.3173]\n",
            "[Epoch 9/10] [Batch 96/107] [D_A loss: 0.1026] [D_B loss: 0.3569] [G loss: 5.6403]\n",
            "[Epoch 9/10] [Batch 97/107] [D_A loss: 0.1935] [D_B loss: 0.5049] [G loss: 3.8223]\n",
            "[Epoch 9/10] [Batch 98/107] [D_A loss: 0.1813] [D_B loss: 0.2643] [G loss: 5.2368]\n",
            "[Epoch 9/10] [Batch 99/107] [D_A loss: 0.2568] [D_B loss: 0.3679] [G loss: 3.9829]\n",
            "[Epoch 9/10] [Batch 100/107] [D_A loss: 0.1654] [D_B loss: 0.2110] [G loss: 6.0210]\n",
            "[Epoch 9/10] [Batch 101/107] [D_A loss: 0.1283] [D_B loss: 0.1709] [G loss: 3.9218]\n",
            "[Epoch 9/10] [Batch 102/107] [D_A loss: 0.3227] [D_B loss: 0.1809] [G loss: 4.1399]\n",
            "[Epoch 9/10] [Batch 103/107] [D_A loss: 0.2302] [D_B loss: 0.2103] [G loss: 3.4365]\n",
            "[Epoch 9/10] [Batch 104/107] [D_A loss: 0.3058] [D_B loss: 0.3815] [G loss: 3.7581]\n",
            "[Epoch 9/10] [Batch 105/107] [D_A loss: 0.2066] [D_B loss: 0.2890] [G loss: 4.2684]\n",
            "[Epoch 9/10] [Batch 106/107] [D_A loss: 0.1641] [D_B loss: 0.1489] [G loss: 3.2414]\n",
            "[Epoch 9/10] [Batch 107/107] [D_A loss: 0.2174] [D_B loss: 0.1011] [G loss: 4.0105]\n",
            "[Epoch 10/10] [Batch 1/107] [D_A loss: 0.0912] [D_B loss: 0.5057] [G loss: 7.6712]\n",
            "[Epoch 10/10] [Batch 2/107] [D_A loss: 0.0411] [D_B loss: 0.2907] [G loss: 6.6785]\n",
            "[Epoch 10/10] [Batch 3/107] [D_A loss: 0.2420] [D_B loss: 0.1651] [G loss: 5.2238]\n",
            "[Epoch 10/10] [Batch 4/107] [D_A loss: 0.2869] [D_B loss: 0.2011] [G loss: 6.6111]\n",
            "[Epoch 10/10] [Batch 5/107] [D_A loss: 0.1834] [D_B loss: 0.1273] [G loss: 4.0077]\n",
            "[Epoch 10/10] [Batch 6/107] [D_A loss: 0.2064] [D_B loss: 0.2454] [G loss: 4.1094]\n",
            "[Epoch 10/10] [Batch 7/107] [D_A loss: 0.1524] [D_B loss: 0.3858] [G loss: 6.2164]\n",
            "[Epoch 10/10] [Batch 8/107] [D_A loss: 0.1665] [D_B loss: 0.1477] [G loss: 4.1245]\n",
            "[Epoch 10/10] [Batch 9/107] [D_A loss: 0.1269] [D_B loss: 0.3050] [G loss: 4.2615]\n",
            "[Epoch 10/10] [Batch 10/107] [D_A loss: 0.1234] [D_B loss: 0.1836] [G loss: 4.2865]\n",
            "[Epoch 10/10] [Batch 11/107] [D_A loss: 0.1396] [D_B loss: 0.2600] [G loss: 4.4704]\n",
            "[Epoch 10/10] [Batch 12/107] [D_A loss: 0.1976] [D_B loss: 0.2007] [G loss: 4.7820]\n",
            "[Epoch 10/10] [Batch 13/107] [D_A loss: 0.1501] [D_B loss: 0.1716] [G loss: 3.8130]\n",
            "[Epoch 10/10] [Batch 14/107] [D_A loss: 0.1328] [D_B loss: 0.2605] [G loss: 3.4380]\n",
            "[Epoch 10/10] [Batch 15/107] [D_A loss: 0.0494] [D_B loss: 0.3525] [G loss: 3.2853]\n",
            "[Epoch 10/10] [Batch 16/107] [D_A loss: 0.2101] [D_B loss: 0.2010] [G loss: 4.1279]\n",
            "[Epoch 10/10] [Batch 17/107] [D_A loss: 0.1594] [D_B loss: 0.2896] [G loss: 4.6263]\n",
            "[Epoch 10/10] [Batch 18/107] [D_A loss: 0.1824] [D_B loss: 0.1307] [G loss: 5.7013]\n",
            "[Epoch 10/10] [Batch 19/107] [D_A loss: 0.1164] [D_B loss: 0.2309] [G loss: 5.7366]\n",
            "[Epoch 10/10] [Batch 20/107] [D_A loss: 0.2019] [D_B loss: 0.2116] [G loss: 5.0123]\n",
            "[Epoch 10/10] [Batch 21/107] [D_A loss: 0.2427] [D_B loss: 0.1826] [G loss: 6.0225]\n",
            "[Epoch 10/10] [Batch 22/107] [D_A loss: 0.4940] [D_B loss: 0.2036] [G loss: 5.4622]\n",
            "[Epoch 10/10] [Batch 23/107] [D_A loss: 0.3508] [D_B loss: 0.1123] [G loss: 6.1436]\n",
            "[Epoch 10/10] [Batch 24/107] [D_A loss: 0.3767] [D_B loss: 0.2219] [G loss: 7.3509]\n",
            "[Epoch 10/10] [Batch 25/107] [D_A loss: 0.3027] [D_B loss: 0.1673] [G loss: 4.9128]\n",
            "[Epoch 10/10] [Batch 26/107] [D_A loss: 0.2449] [D_B loss: 0.0750] [G loss: 4.6253]\n",
            "[Epoch 10/10] [Batch 27/107] [D_A loss: 0.5381] [D_B loss: 0.2778] [G loss: 5.3869]\n",
            "[Epoch 10/10] [Batch 28/107] [D_A loss: 0.3141] [D_B loss: 0.0818] [G loss: 4.6857]\n",
            "[Epoch 10/10] [Batch 29/107] [D_A loss: 0.4641] [D_B loss: 0.2921] [G loss: 4.6108]\n",
            "[Epoch 10/10] [Batch 30/107] [D_A loss: 0.3180] [D_B loss: 0.2591] [G loss: 4.3401]\n",
            "[Epoch 10/10] [Batch 31/107] [D_A loss: 0.3093] [D_B loss: 0.3333] [G loss: 4.8184]\n",
            "[Epoch 10/10] [Batch 32/107] [D_A loss: 0.1782] [D_B loss: 0.2195] [G loss: 5.2287]\n",
            "[Epoch 10/10] [Batch 33/107] [D_A loss: 0.2595] [D_B loss: 0.4344] [G loss: 4.5839]\n",
            "[Epoch 10/10] [Batch 34/107] [D_A loss: 0.2362] [D_B loss: 0.2083] [G loss: 4.1482]\n",
            "[Epoch 10/10] [Batch 35/107] [D_A loss: 0.2341] [D_B loss: 0.1294] [G loss: 4.0952]\n",
            "[Epoch 10/10] [Batch 36/107] [D_A loss: 0.1924] [D_B loss: 0.1185] [G loss: 5.3501]\n",
            "[Epoch 10/10] [Batch 37/107] [D_A loss: 0.1862] [D_B loss: 0.2579] [G loss: 5.8057]\n",
            "[Epoch 10/10] [Batch 38/107] [D_A loss: 0.2266] [D_B loss: 0.1189] [G loss: 10.5967]\n",
            "[Epoch 10/10] [Batch 39/107] [D_A loss: 0.2331] [D_B loss: 0.1622] [G loss: 3.9771]\n",
            "[Epoch 10/10] [Batch 40/107] [D_A loss: 0.1782] [D_B loss: 0.4034] [G loss: 4.1173]\n",
            "[Epoch 10/10] [Batch 41/107] [D_A loss: 0.1556] [D_B loss: 0.2829] [G loss: 5.0681]\n",
            "[Epoch 10/10] [Batch 42/107] [D_A loss: 0.1895] [D_B loss: 0.3370] [G loss: 6.5392]\n",
            "[Epoch 10/10] [Batch 43/107] [D_A loss: 0.2006] [D_B loss: 0.4026] [G loss: 8.8010]\n",
            "[Epoch 10/10] [Batch 44/107] [D_A loss: 0.1372] [D_B loss: 0.2788] [G loss: 4.3889]\n",
            "[Epoch 10/10] [Batch 45/107] [D_A loss: 0.1299] [D_B loss: 0.2493] [G loss: 4.3392]\n",
            "[Epoch 10/10] [Batch 46/107] [D_A loss: 0.1570] [D_B loss: 0.1774] [G loss: 6.4131]\n",
            "[Epoch 10/10] [Batch 47/107] [D_A loss: 0.1915] [D_B loss: 0.2461] [G loss: 4.3208]\n",
            "[Epoch 10/10] [Batch 48/107] [D_A loss: 0.1921] [D_B loss: 0.1214] [G loss: 4.6246]\n",
            "[Epoch 10/10] [Batch 49/107] [D_A loss: 0.1110] [D_B loss: 0.3270] [G loss: 4.8241]\n",
            "[Epoch 10/10] [Batch 50/107] [D_A loss: 0.1726] [D_B loss: 0.2700] [G loss: 5.5661]\n",
            "[Epoch 10/10] [Batch 51/107] [D_A loss: 0.2601] [D_B loss: 0.1448] [G loss: 3.6149]\n",
            "[Epoch 10/10] [Batch 52/107] [D_A loss: 0.0743] [D_B loss: 0.2946] [G loss: 6.2896]\n",
            "[Epoch 10/10] [Batch 53/107] [D_A loss: 0.1649] [D_B loss: 0.2112] [G loss: 13.7484]\n",
            "[Epoch 10/10] [Batch 54/107] [D_A loss: 0.1240] [D_B loss: 0.1663] [G loss: 5.5539]\n",
            "[Epoch 10/10] [Batch 55/107] [D_A loss: 0.2456] [D_B loss: 0.2217] [G loss: 3.9037]\n",
            "[Epoch 10/10] [Batch 56/107] [D_A loss: 0.2811] [D_B loss: 0.3642] [G loss: 6.4517]\n",
            "[Epoch 10/10] [Batch 57/107] [D_A loss: 0.0922] [D_B loss: 0.1073] [G loss: 6.3129]\n",
            "[Epoch 10/10] [Batch 58/107] [D_A loss: 0.1222] [D_B loss: 0.1342] [G loss: 4.4184]\n",
            "[Epoch 10/10] [Batch 59/107] [D_A loss: 0.1597] [D_B loss: 0.1687] [G loss: 5.4879]\n",
            "[Epoch 10/10] [Batch 60/107] [D_A loss: 0.0891] [D_B loss: 0.2256] [G loss: 3.8344]\n",
            "[Epoch 10/10] [Batch 61/107] [D_A loss: 0.1766] [D_B loss: 0.3140] [G loss: 4.1069]\n",
            "[Epoch 10/10] [Batch 62/107] [D_A loss: 0.2654] [D_B loss: 0.2174] [G loss: 3.7679]\n",
            "[Epoch 10/10] [Batch 63/107] [D_A loss: 0.2260] [D_B loss: 0.2435] [G loss: 5.2689]\n",
            "[Epoch 10/10] [Batch 64/107] [D_A loss: 0.1648] [D_B loss: 0.1079] [G loss: 6.2283]\n",
            "[Epoch 10/10] [Batch 65/107] [D_A loss: 0.1753] [D_B loss: 0.4716] [G loss: 3.7033]\n",
            "[Epoch 10/10] [Batch 66/107] [D_A loss: 0.3529] [D_B loss: 0.3545] [G loss: 5.0595]\n",
            "[Epoch 10/10] [Batch 67/107] [D_A loss: 0.1262] [D_B loss: 0.3005] [G loss: 4.6467]\n",
            "[Epoch 10/10] [Batch 68/107] [D_A loss: 0.1941] [D_B loss: 0.2099] [G loss: 4.7191]\n",
            "[Epoch 10/10] [Batch 69/107] [D_A loss: 0.2510] [D_B loss: 0.1131] [G loss: 5.0088]\n",
            "[Epoch 10/10] [Batch 70/107] [D_A loss: 0.1610] [D_B loss: 0.1316] [G loss: 6.1267]\n",
            "[Epoch 10/10] [Batch 71/107] [D_A loss: 0.1427] [D_B loss: 0.3792] [G loss: 3.8773]\n",
            "[Epoch 10/10] [Batch 72/107] [D_A loss: 0.1549] [D_B loss: 0.2909] [G loss: 3.5413]\n",
            "[Epoch 10/10] [Batch 73/107] [D_A loss: 0.1111] [D_B loss: 0.2273] [G loss: 3.7236]\n",
            "[Epoch 10/10] [Batch 74/107] [D_A loss: 0.0742] [D_B loss: 0.1121] [G loss: 4.5923]\n",
            "[Epoch 10/10] [Batch 75/107] [D_A loss: 0.1728] [D_B loss: 0.3534] [G loss: 4.5122]\n",
            "[Epoch 10/10] [Batch 76/107] [D_A loss: 0.2364] [D_B loss: 0.2851] [G loss: 5.3987]\n",
            "[Epoch 10/10] [Batch 77/107] [D_A loss: 0.1173] [D_B loss: 0.0722] [G loss: 6.5327]\n",
            "[Epoch 10/10] [Batch 78/107] [D_A loss: 0.2483] [D_B loss: 0.2445] [G loss: 4.5098]\n",
            "[Epoch 10/10] [Batch 79/107] [D_A loss: 0.1459] [D_B loss: 0.3606] [G loss: 5.0619]\n",
            "[Epoch 10/10] [Batch 80/107] [D_A loss: 0.2990] [D_B loss: 0.3196] [G loss: 3.6280]\n",
            "[Epoch 10/10] [Batch 81/107] [D_A loss: 0.1198] [D_B loss: 0.1664] [G loss: 4.8724]\n",
            "[Epoch 10/10] [Batch 82/107] [D_A loss: 0.1826] [D_B loss: 0.1206] [G loss: 4.4862]\n",
            "[Epoch 10/10] [Batch 83/107] [D_A loss: 0.1803] [D_B loss: 0.2490] [G loss: 3.9088]\n",
            "[Epoch 10/10] [Batch 84/107] [D_A loss: 0.1999] [D_B loss: 0.2897] [G loss: 3.9056]\n",
            "[Epoch 10/10] [Batch 85/107] [D_A loss: 0.1040] [D_B loss: 0.0633] [G loss: 6.0871]\n",
            "[Epoch 10/10] [Batch 86/107] [D_A loss: 0.0526] [D_B loss: 0.3207] [G loss: 4.5437]\n",
            "[Epoch 10/10] [Batch 87/107] [D_A loss: 0.1792] [D_B loss: 0.2298] [G loss: 5.4320]\n",
            "[Epoch 10/10] [Batch 88/107] [D_A loss: 0.0706] [D_B loss: 0.2616] [G loss: 6.6143]\n",
            "[Epoch 10/10] [Batch 89/107] [D_A loss: 0.0523] [D_B loss: 0.1227] [G loss: 5.6656]\n",
            "[Epoch 10/10] [Batch 90/107] [D_A loss: 0.2824] [D_B loss: 0.1167] [G loss: 4.0087]\n",
            "[Epoch 10/10] [Batch 91/107] [D_A loss: 0.1168] [D_B loss: 0.1479] [G loss: 4.4100]\n",
            "[Epoch 10/10] [Batch 92/107] [D_A loss: 0.2254] [D_B loss: 0.1162] [G loss: 7.1754]\n",
            "[Epoch 10/10] [Batch 93/107] [D_A loss: 0.1444] [D_B loss: 0.2902] [G loss: 5.7974]\n",
            "[Epoch 10/10] [Batch 94/107] [D_A loss: 0.1031] [D_B loss: 0.2472] [G loss: 3.4795]\n",
            "[Epoch 10/10] [Batch 95/107] [D_A loss: 0.2252] [D_B loss: 0.1743] [G loss: 3.9116]\n",
            "[Epoch 10/10] [Batch 96/107] [D_A loss: 0.0515] [D_B loss: 0.2202] [G loss: 4.1554]\n",
            "[Epoch 10/10] [Batch 97/107] [D_A loss: 0.3513] [D_B loss: 0.1363] [G loss: 5.3736]\n",
            "[Epoch 10/10] [Batch 98/107] [D_A loss: 0.2203] [D_B loss: 0.1654] [G loss: 5.2428]\n",
            "[Epoch 10/10] [Batch 99/107] [D_A loss: 0.2908] [D_B loss: 0.0900] [G loss: 3.2119]\n",
            "[Epoch 10/10] [Batch 100/107] [D_A loss: 0.2061] [D_B loss: 0.2638] [G loss: 3.9445]\n",
            "[Epoch 10/10] [Batch 101/107] [D_A loss: 0.2974] [D_B loss: 0.1125] [G loss: 4.0157]\n",
            "[Epoch 10/10] [Batch 102/107] [D_A loss: 0.1688] [D_B loss: 0.2301] [G loss: 7.2660]\n",
            "[Epoch 10/10] [Batch 103/107] [D_A loss: 0.2649] [D_B loss: 0.1481] [G loss: 4.7403]\n",
            "[Epoch 10/10] [Batch 104/107] [D_A loss: 0.1349] [D_B loss: 0.1412] [G loss: 6.3432]\n",
            "[Epoch 10/10] [Batch 105/107] [D_A loss: 0.2298] [D_B loss: 0.2668] [G loss: 4.5097]\n",
            "[Epoch 10/10] [Batch 106/107] [D_A loss: 0.1532] [D_B loss: 0.1683] [G loss: 6.4296]\n",
            "[Epoch 10/10] [Batch 107/107] [D_A loss: 0.2299] [D_B loss: 0.2447] [G loss: 3.4256]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNkZGRm5gwQ7ivt4sXScHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}